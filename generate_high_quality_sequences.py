#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç”Ÿæˆé«˜è´¨é‡æŠ—èŒè‚½åºåˆ—è„šæœ¬

åŠŸèƒ½:
1. ä½¿ç”¨å¤šæ ·é‡‡æ ·ç”Ÿæˆåºåˆ—
2. ç”¨é¢„æµ‹æ¨¡å‹ç­›é€‰æ¦‚ç‡>0.95çš„åºåˆ—
3. ä¸ç°æœ‰æ•°æ®åº“å»é‡
4. è¾“å‡ºé«˜è´¨é‡çš„FASTAæ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
python generate_high_quality_sequences.py --num_batches 10 --batch_size 50 --output generated_sequences.fasta

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-06-10
"""

import os
import sys
import torch
import numpy as np
import pickle
import logging
import argparse
from datetime import datetime
from typing import List, Set, Tuple
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler
from peptides import Peptide

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
ENHANCED_ARCH_PATH = os.path.join(PROJECT_ROOT, 'enhanced_architecture')
sys.path.append(PROJECT_ROOT)
sys.path.append(ENHANCED_ARCH_PATH)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from enhanced_architecture.config.model_config import get_config
    from enhanced_architecture.esm2_auxiliary_encoder import ESM2AuxiliaryEncoder
    from enhanced_architecture.diffusion_models.d3pm_diffusion import D3PMDiffusion, D3PMScheduler, D3PMUNet
    from enhanced_architecture.data_loader import tokens_to_sequence
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'generation_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é¢„æµ‹ç›¸å…³å¸¸é‡ (ä»hybrid_predict.pyå¤åˆ¶)
AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'
VOCAB_DICT = {aa: i+2 for i, aa in enumerate(AMINO_ACIDS)}
VOCAB_DICT['<PAD>'] = 0
VOCAB_DICT['<UNK>'] = 1
VOCAB_SIZE = len(VOCAB_DICT)
MAX_SEQUENCE_LENGTH = 32
PAD_TOKEN_ID = VOCAB_DICT['<PAD>']
UNK_TOKEN_ID = VOCAB_DICT['<UNK>']

class SequenceGenerator:
    """åºåˆ—ç”Ÿæˆå™¨"""
    
    def __init__(self, checkpoint_path: str, config_name: str = "dual_4090"):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config = get_config(config_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åŠ è½½æ¨¡å‹
        self.load_models(checkpoint_path)
        logger.info(f"âœ… åºåˆ—ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def load_models(self, checkpoint_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        logger.info(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        
        # åˆå§‹åŒ–ESM-2ç¼–ç å™¨
        self.esm2_encoder = ESM2AuxiliaryEncoder(self.config.esm2)
        self.esm2_encoder.to(self.device)
        
        # åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹
        self.unet = D3PMUNet(
            vocab_size=self.config.diffusion.vocab_size,
            hidden_size=self.config.diffusion.hidden_size,
            n_layers=self.config.diffusion.n_layers,
            n_heads=self.config.diffusion.n_heads,
            max_seq_len=self.config.diffusion.max_seq_len,
            time_embed_dim=self.config.diffusion.time_embed_dim,
            esm_feature_dim=self.config.esm2.feature_dim
        )
        
        self.scheduler = D3PMScheduler(
            num_classes=self.config.diffusion.vocab_size,
            num_timesteps=self.config.diffusion.num_timesteps
        )
        
        self.diffusion_model = D3PMDiffusion(
            unet=self.unet,
            scheduler=self.scheduler,
            vocab_size=self.config.diffusion.vocab_size
        )
        
        # åŠ è½½æƒé‡
        self.esm2_encoder.load_state_dict(checkpoint['esm2_encoder_state_dict'])
        self.diffusion_model.load_state_dict(checkpoint['diffusion_model_state_dict'])
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.esm2_encoder.eval()
        self.diffusion_model.eval()
        
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def generate_sequences(self, num_sequences: int, seq_length: int = 40, 
                          temperature: float = 1.0, diversity_strength: float = 1.5) -> List[str]:
        """ç”Ÿæˆåºåˆ—"""
        logger.info(f"ğŸ§¬ å¼€å§‹ç”Ÿæˆ {num_sequences} ä¸ªåºåˆ—ï¼Œé•¿åº¦: {seq_length}")
        
        with torch.no_grad():
            # å‡†å¤‡ESMç‰¹å¾ï¼ˆè¿™é‡Œä½¿ç”¨é›¶å‘é‡ï¼Œå®é™…å¯ä»¥ç”¨å‚è€ƒåºåˆ—ï¼‰
            esm_features = torch.zeros(
                num_sequences, 
                self.config.esm2.feature_dim, 
                device=self.device
            )
            
            # ä½¿ç”¨å¤šæ ·é‡‡æ ·
            generated_tokens = self.diffusion_model.diverse_sample(
                batch_size=num_sequences,
                seq_len=seq_length,
                esm_features=esm_features,
                diversity_strength=diversity_strength,
                temperature=temperature
            )
        
        # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
        sequences = []
        for i, tokens in enumerate(generated_tokens):
            seq = tokens_to_sequence(tokens.cpu().numpy())
            if seq and len(seq) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„åºåˆ—
                sequences.append(seq)
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(sequences)} ä¸ªæœ‰æ•ˆåºåˆ—")
        return sequences

class SequencePredictor:
    """åºåˆ—é¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str, scaler_path: str):
        """åˆå§‹åŒ–é¢„æµ‹å™¨"""
        logger.info("ğŸ“Š åˆå§‹åŒ–é¢„æµ‹å™¨...")
        
        # åŠ è½½æ¨¡å‹
        self.model = tf.keras.models.load_model(model_path)
        logger.info(f"âœ… åŠ è½½é¢„æµ‹æ¨¡å‹: {model_path}")
        
        # åŠ è½½ç‰¹å¾æ ‡å‡†åŒ–å™¨
        with open(scaler_path, 'rb') as f:
            self.scaler = pickle.load(f)
        logger.info(f"âœ… åŠ è½½æ ‡å‡†åŒ–å™¨: {scaler_path}")
    
    def predict_sequences(self, sequences: List[str], threshold: float = 0.95) -> List[Tuple[str, float]]:
        """é¢„æµ‹åºåˆ—å¹¶ç­›é€‰é«˜è´¨é‡åºåˆ—"""
        logger.info(f"ğŸ” å¼€å§‹é¢„æµ‹ {len(sequences)} ä¸ªåºåˆ—ï¼Œé˜ˆå€¼: {threshold}")
        
        if not sequences:
            return []
        
        # æå–ç‰¹å¾
        features_data = []
        valid_sequences = []
        
        for seq in sequences:
            try:
                # åºåˆ—ç¼–ç 
                encoded_seq = [VOCAB_DICT.get(aa, UNK_TOKEN_ID) for aa in seq]
                padded_seq = pad_sequences([encoded_seq], maxlen=MAX_SEQUENCE_LENGTH, 
                                         padding='post', value=PAD_TOKEN_ID)[0]
                
                # å…¨å±€ç‰¹å¾è®¡ç®—
                peptide = Peptide(seq)
                global_features = [
                    len(seq),
                    peptide.molecular_weight(),
                    peptide.aromaticity(),
                    peptide.instability_index(),
                    peptide.aliphatic_index(),
                    peptide.boman_index(),
                    peptide.hydrophobic_ratio(),
                    peptide.charge(pH=7.0),
                    peptide.charge_density(pH=7.0),
                    peptide.isoelectric_point()
                ]
                
                # æ£€æŸ¥ç‰¹å¾æœ‰æ•ˆæ€§
                if any(np.isnan(global_features) or np.isinf(global_features)):
                    logger.debug(f"è·³è¿‡æ— æ•ˆç‰¹å¾åºåˆ—: {seq[:20]}...")
                    continue
                
                features_data.append({
                    'sequence': seq,
                    'encoded_seq': padded_seq,
                    'global_features': global_features
                })
                valid_sequences.append(seq)
                
            except Exception as e:
                logger.debug(f"ç‰¹å¾æå–å¤±è´¥ {seq[:20]}...: {e}")
                continue
        
        if not features_data:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„åºåˆ—ç”¨äºé¢„æµ‹")
            return []
        
        # å‡†å¤‡é¢„æµ‹æ•°æ®
        X_seq = np.array([item['encoded_seq'] for item in features_data])
        X_global = np.array([item['global_features'] for item in features_data])
        
        # æ ‡å‡†åŒ–å…¨å±€ç‰¹å¾
        X_global_scaled = self.scaler.transform(X_global)
        
        # é¢„æµ‹
        predictions = self.model.predict([X_seq, X_global_scaled], verbose=0)
        
        # ç­›é€‰é«˜è´¨é‡åºåˆ—
        high_quality_sequences = []
        for i, prob in enumerate(predictions.flatten()):
            if prob >= threshold:
                high_quality_sequences.append((valid_sequences[i], float(prob)))
        
        logger.info(f"âœ… ç­›é€‰å‡º {len(high_quality_sequences)} ä¸ªé«˜è´¨é‡åºåˆ— (â‰¥{threshold})")
        return high_quality_sequences

class SequenceDeduplicator:
    """åºåˆ—å»é‡å™¨"""
    
    def __init__(self, database_fasta_path: str):
        """åˆå§‹åŒ–å»é‡å™¨"""
        logger.info("ğŸ”„ åˆå§‹åŒ–å»é‡å™¨...")
        self.existing_sequences = self.load_existing_sequences(database_fasta_path)
        logger.info(f"âœ… åŠ è½½äº† {len(self.existing_sequences)} ä¸ªç°æœ‰åºåˆ—")
    
    def load_existing_sequences(self, fasta_path: str) -> Set[str]:
        """åŠ è½½ç°æœ‰æ•°æ®åº“ä¸­çš„åºåˆ—"""
        sequences = set()
        try:
            for record in SeqIO.parse(fasta_path, "fasta"):
                seq = str(record.seq).upper()
                sequences.add(seq)
        except FileNotFoundError:
            logger.warning(f"æ•°æ®åº“æ–‡ä»¶æœªæ‰¾åˆ°: {fasta_path}")
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®åº“åºåˆ—å¤±è´¥: {e}")
        return sequences
    
    def deduplicate(self, sequences: List[Tuple[str, float]]) -> List[Tuple[str, float]]:
        """å»é‡åºåˆ—"""
        logger.info(f"ğŸ”„ å¼€å§‹å»é‡ {len(sequences)} ä¸ªåºåˆ—...")
        
        unique_sequences = []
        seen_sequences = set()
        
        for seq, prob in sequences:
            seq_upper = seq.upper()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ç”Ÿæˆçš„åºåˆ—é‡å¤
            if seq_upper in seen_sequences:
                logger.debug(f"é‡å¤çš„ç”Ÿæˆåºåˆ—: {seq[:20]}...")
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸æ•°æ®åº“åºåˆ—é‡å¤
            if seq_upper in self.existing_sequences:
                logger.debug(f"ä¸æ•°æ®åº“é‡å¤: {seq[:20]}...")
                continue
            
            unique_sequences.append((seq, prob))
            seen_sequences.add(seq_upper)
        
        logger.info(f"âœ… å»é‡å®Œæˆï¼Œä¿ç•™ {len(unique_sequences)} ä¸ªå”¯ä¸€åºåˆ—")
        return unique_sequences

def save_sequences_to_fasta(sequences: List[Tuple[str, float]], output_path: str):
    """ä¿å­˜åºåˆ—åˆ°FASTAæ–‡ä»¶"""
    logger.info(f"ğŸ’¾ ä¿å­˜ {len(sequences)} ä¸ªåºåˆ—åˆ°: {output_path}")
    
    records = []
    for i, (seq, prob) in enumerate(sequences, 1):
        record_id = f"Generated_AMP_{i:06d}"
        description = f"Probability: {prob:.4f} | Length: {len(seq)} | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        record = SeqRecord(Seq(seq), id=record_id, description=description)
        records.append(record)
    
    SeqIO.write(records, output_path, "fasta")
    logger.info(f"âœ… åºåˆ—å·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆé«˜è´¨é‡æŠ—èŒè‚½åºåˆ—")
    parser.add_argument("--num_batches", type=int, default=10, help="ç”Ÿæˆæ‰¹æ¬¡æ•°")
    parser.add_argument("--batch_size", type=int, default=50, help="æ¯æ‰¹æ¬¡åºåˆ—æ•°é‡")
    parser.add_argument("--seq_length", type=int, default=40, help="åºåˆ—é•¿åº¦")
    parser.add_argument("--threshold", type=float, default=0.95, help="é¢„æµ‹æ¦‚ç‡é˜ˆå€¼")
    parser.add_argument("--temperature", type=float, default=1.0, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--diversity_strength", type=float, default=1.5, help="å¤šæ ·æ€§å¼ºåº¦")
    parser.add_argument("--output", type=str, default="generated_high_quality_sequences.fasta", help="è¾“å‡ºFASTAæ–‡ä»¶")
    parser.add_argument("--checkpoint", type=str, default="enhanced_architecture/output/checkpoints/best.pt", help="ç”Ÿæˆæ¨¡å‹æ£€æŸ¥ç‚¹")
    parser.add_argument("--predict_model", type=str, default="model/hybrid_classifier_best_tuned.keras", help="é¢„æµ‹æ¨¡å‹è·¯å¾„")
    parser.add_argument("--scaler", type=str, default="model/hybrid_model_scaler.pkl", help="æ ‡å‡†åŒ–å™¨è·¯å¾„")
    parser.add_argument("--database", type=str, default="data/Gram+-.fasta", help="ç°æœ‰æ•°æ®åº“FASTAæ–‡ä»¶")
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆé«˜è´¨é‡æŠ—èŒè‚½åºåˆ—")
    logger.info(f"ğŸ“Š å‚æ•°: æ‰¹æ¬¡æ•°={args.num_batches}, æ‰¹æ¬¡å¤§å°={args.batch_size}, é˜ˆå€¼={args.threshold}")
    
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        generator = SequenceGenerator(args.checkpoint)
        predictor = SequencePredictor(args.predict_model, args.scaler)
        deduplicator = SequenceDeduplicator(args.database)
        
        all_high_quality_sequences = []
        
        # åˆ†æ‰¹ç”Ÿæˆå’Œç­›é€‰
        for batch_idx in range(args.num_batches):
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{args.num_batches}")
            
            # ç”Ÿæˆåºåˆ—
            generated_sequences = generator.generate_sequences(
                num_sequences=args.batch_size,
                seq_length=args.seq_length,
                temperature=args.temperature,
                diversity_strength=args.diversity_strength
            )
            
            if not generated_sequences:
                logger.warning(f"æ‰¹æ¬¡ {batch_idx + 1} æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆåºåˆ—")
                continue
            
            # é¢„æµ‹ç­›é€‰
            high_quality_batch = predictor.predict_sequences(
                generated_sequences, 
                threshold=args.threshold
            )
            
            if high_quality_batch:
                all_high_quality_sequences.extend(high_quality_batch)
                logger.info(f"âœ… æ‰¹æ¬¡ {batch_idx + 1} ç­›é€‰å‡º {len(high_quality_batch)} ä¸ªé«˜è´¨é‡åºåˆ—")
            else:
                logger.info(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} æ²¡æœ‰ç¬¦åˆé˜ˆå€¼çš„åºåˆ—")
        
        if not all_high_quality_sequences:
            logger.error("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•é«˜è´¨é‡åºåˆ—ï¼Œè¯·è°ƒæ•´å‚æ•°")
            return
        
        logger.info(f"ğŸ¯ æ€»å…±ç­›é€‰å‡º {len(all_high_quality_sequences)} ä¸ªé«˜è´¨é‡åºåˆ—")
        
        # å»é‡
        unique_sequences = deduplicator.deduplicate(all_high_quality_sequences)
        
        if not unique_sequences:
            logger.error("âŒ å»é‡åæ²¡æœ‰å‰©ä½™åºåˆ—")
            return
        
        # æŒ‰æ¦‚ç‡æ’åº
        unique_sequences.sort(key=lambda x: x[1], reverse=True)
        
        # ä¿å­˜ç»“æœ
        save_sequences_to_fasta(unique_sequences, args.output)
        
        # ç»Ÿè®¡ä¿¡æ¯
        probabilities = [prob for _, prob in unique_sequences]
        logger.info("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f"  - å”¯ä¸€åºåˆ—æ•°: {len(unique_sequences)}")
        logger.info(f"  - å¹³å‡æ¦‚ç‡: {np.mean(probabilities):.4f}")
        logger.info(f"  - æœ€é«˜æ¦‚ç‡: {np.max(probabilities):.4f}")
        logger.info(f"  - æœ€ä½æ¦‚ç‡: {np.min(probabilities):.4f}")
        logger.info(f"  - è¾“å‡ºæ–‡ä»¶: {args.output}")
        
        logger.info("ğŸ‰ ç”Ÿæˆå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
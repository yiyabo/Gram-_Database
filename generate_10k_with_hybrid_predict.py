#!/usr/bin/env python3
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„ç”Ÿæˆå™¨å’Œhybrid_predict.pyç”Ÿæˆ10000æ¡åºåˆ—å¹¶è®¡ç®—çœŸå®é¢„æµ‹å¾—åˆ†
- ç”Ÿæˆå™¨ï¼šä½¿ç”¨æ‚¨è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹
- åˆ†ç±»å™¨ï¼šè°ƒç”¨hybrid_predict.pyè„šæœ¬
"""

import os
import sys
import torch
import numpy as np
import pandas as pd
from pathlib import Path
import argparse
from tqdm import tqdm
import tempfile
import subprocess
import logging
from typing import List, Tuple
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'enhanced_architecture'))

# å¯¼å…¥ç”Ÿæˆå™¨æ¨¡å—
from enhanced_architecture.config.model_config import get_config
from enhanced_architecture.esm2_auxiliary_encoder import ESM2AuxiliaryEncoder
from enhanced_architecture.diffusion_models.d3pm_diffusion import D3PMDiffusion, D3PMScheduler, D3PMUNet
from enhanced_architecture.data_loader import tokens_to_sequence

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HybridSequenceGenerator:
    """ä½¿ç”¨ç”Ÿæˆå™¨ + hybrid_predict.py çš„åºåˆ—ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 generator_checkpoint: str,
                 hybrid_predict_script: str = "hybrid_predict.py",
                 model_weights: str = "model/best_weights.h5",
                 scaler_path: str = "model/scaler.pkl", 
                 feature_names_file: str = "data/feature_names.txt",
                 config_name: str = "dual_4090"):
        """
        åˆå§‹åŒ–æ··åˆç”Ÿæˆå™¨
        
        Args:
            generator_checkpoint: ç”Ÿæˆå™¨æ£€æŸ¥ç‚¹è·¯å¾„
            hybrid_predict_script: hybrid_predict.pyè„šæœ¬è·¯å¾„
            model_weights: åˆ†ç±»å™¨æƒé‡è·¯å¾„
            scaler_path: æ ‡å‡†åŒ–å™¨è·¯å¾„
            feature_names_file: ç‰¹å¾åç§°æ–‡ä»¶è·¯å¾„
            config_name: é…ç½®åç§°
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.generator_checkpoint = generator_checkpoint
        self.hybrid_predict_script = hybrid_predict_script
        self.model_weights = model_weights
        self.scaler_path = scaler_path
        self.feature_names_file = feature_names_file
        self.config_name = config_name
        
        # æ¨¡å‹ç»„ä»¶
        self.generator_model = None
        self.config = None
        
        logger.info(f"åˆå§‹åŒ–æ··åˆç”Ÿæˆå™¨ï¼Œè®¾å¤‡: {self.device}")
        logger.info(f"ç”Ÿæˆå™¨æ£€æŸ¥ç‚¹: {self.generator_checkpoint}")
        logger.info(f"åˆ†ç±»å™¨è„šæœ¬: {self.hybrid_predict_script}")
    
    def load_generator(self):
        """åŠ è½½ç”Ÿæˆå™¨æ¨¡å‹"""
        logger.info(f"åŠ è½½ç”Ÿæˆå™¨: {self.generator_checkpoint}")
        
        if not os.path.exists(self.generator_checkpoint):
            raise FileNotFoundError(f"ç”Ÿæˆå™¨æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {self.generator_checkpoint}")
        
        # åŠ è½½é…ç½®
        self.config = get_config(self.config_name)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(self.generator_checkpoint, map_location=self.device, weights_only=False)
        
        # åˆå§‹åŒ–ESM-2ç¼–ç å™¨
        self.esm2_encoder = ESM2AuxiliaryEncoder(self.config.esm2)
        if 'esm2_encoder_state_dict' in checkpoint:
            self.esm2_encoder.load_state_dict(checkpoint['esm2_encoder_state_dict'])
        self.esm2_encoder.to(self.device)
        self.esm2_encoder.eval()
        
        # åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹
        scheduler = D3PMScheduler(
            num_timesteps=self.config.diffusion.num_timesteps,
            schedule_type=self.config.diffusion.schedule_type,
            vocab_size=self.config.diffusion.vocab_size
        )
        
        unet = D3PMUNet(
            vocab_size=self.config.diffusion.vocab_size,
            max_seq_len=self.config.diffusion.max_seq_len,
            hidden_dim=self.config.diffusion.hidden_dim,
            num_layers=self.config.diffusion.num_layers,
            num_heads=self.config.diffusion.num_heads,
            dropout=self.config.diffusion.dropout
        )
        
        if 'diffusion_model_state_dict' in checkpoint:
            unet.load_state_dict(checkpoint['diffusion_model_state_dict'])
        
        self.generator_model = D3PMDiffusion(
            model=unet,
            scheduler=scheduler,
            device=self.device
        )
        
        logger.info("âœ… ç”Ÿæˆå™¨åŠ è½½æˆåŠŸ")
    
    def check_classifier_dependencies(self):
        """æ£€æŸ¥åˆ†ç±»å™¨ä¾èµ–æ–‡ä»¶"""
        dependencies = [
            self.hybrid_predict_script,
            self.model_weights,
            self.scaler_path,
            self.feature_names_file
        ]
        
        missing_files = []
        for file_path in dependencies:
            if not os.path.exists(file_path):
                missing_files.append(file_path)
        
        if missing_files:
            logger.error(f"ç¼ºå°‘åˆ†ç±»å™¨ä¾èµ–æ–‡ä»¶: {missing_files}")
            raise FileNotFoundError(f"ç¼ºå°‘åˆ†ç±»å™¨ä¾èµ–æ–‡ä»¶: {missing_files}")
        
        logger.info("âœ… åˆ†ç±»å™¨ä¾èµ–æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    def generate_batch_sequences(self, batch_size: int = 50, seq_length: int = 50,
                               temperature: float = 0.8) -> List[str]:
        """
        ç”Ÿæˆä¸€æ‰¹åºåˆ—
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            seq_length: åºåˆ—é•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
        
        Returns:
            ç”Ÿæˆçš„åºåˆ—åˆ—è¡¨
        """
        with torch.no_grad():
            # ä½¿ç”¨å¤šç§é‡‡æ ·ç­–ç•¥å¢åŠ å¤šæ ·æ€§
            strategy = np.random.choice(['standard', 'low_temp', 'high_temp'], p=[0.5, 0.3, 0.2])
            
            if strategy == 'standard':
                generated_tokens = self.generator_model.sample(
                    batch_size=batch_size,
                    seq_len=seq_length,
                    temperature=temperature
                )
            elif strategy == 'low_temp':
                generated_tokens = self.generator_model.sample(
                    batch_size=batch_size,
                    seq_len=seq_length,
                    temperature=temperature * 0.7
                )
            else:  # high_temp
                generated_tokens = self.generator_model.sample(
                    batch_size=batch_size,
                    seq_len=seq_length,
                    temperature=temperature * 1.3
                )
        
        # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
        sequences = []
        for tokens in generated_tokens:
            seq = tokens_to_sequence(tokens.cpu().numpy())
            # è¿‡æ»¤æ‰å¤ªçŸ­çš„åºåˆ—
            if len(seq) >= 10:
                sequences.append(seq)
        
        return sequences
    
    def predict_sequences_with_hybrid_script(self, sequences: List[str]) -> Tuple[List[float], List[int]]:
        """
        ä½¿ç”¨hybrid_predict.pyè„šæœ¬é¢„æµ‹åºåˆ—
        
        Args:
            sequences: åºåˆ—åˆ—è¡¨
        
        Returns:
            (æ¦‚ç‡åˆ—è¡¨, é¢„æµ‹æ ‡ç­¾åˆ—è¡¨)
        """
        if not sequences:
            return [], []
        
        # åˆ›å»ºä¸´æ—¶FASTAæ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:
            for i, seq in enumerate(sequences):
                f.write(f">Generated_Seq_{i+1}\n{seq}\n")
            temp_fasta = f.name
        
        # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
        temp_output = tempfile.mktemp(suffix='.txt')
        
        try:
            # æ„å»ºhybrid_predict.pyå‘½ä»¤
            cmd = [
                sys.executable, self.hybrid_predict_script,
                "--model_path", self.model_weights,
                "--fasta_file", temp_fasta,
                "--scaler_path", self.scaler_path,
                "--output_file", temp_output,
                "--threshold", "0.5",
                "--feature_names_file", self.feature_names_file
            ]
            
            # æ‰§è¡Œé¢„æµ‹è„šæœ¬
            logger.debug(f"æ‰§è¡Œé¢„æµ‹å‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                logger.warning(f"hybrid_predict.pyæ‰§è¡Œå¤±è´¥: {result.stderr}")
                return [], []
            
            # è¯»å–é¢„æµ‹ç»“æœ
            if not os.path.exists(temp_output):
                logger.warning("é¢„æµ‹è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                return [], []
            
            probabilities = []
            predictions = []
            
            with open(temp_output, 'r') as f:
                lines = f.readlines()
                for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                    parts = line.strip().split('\t')
                    if len(parts) >= 3:
                        pred_label = parts[1]
                        prob_str = parts[2]
                        
                        try:
                            probability = float(prob_str)
                            prediction = 1 if "æŠ—é©å…°æ°é˜´æ€§èŒæ´»æ€§" in pred_label else 0
                            
                            probabilities.append(probability)
                            predictions.append(prediction)
                        except ValueError:
                            logger.warning(f"è§£æé¢„æµ‹ç»“æœå¤±è´¥: {line.strip()}")
                            probabilities.append(0.0)
                            predictions.append(0)
            
            return probabilities, predictions
            
        except subprocess.TimeoutExpired:
            logger.error("hybrid_predict.pyæ‰§è¡Œè¶…æ—¶")
            return [], []
        except Exception as e:
            logger.error(f"è°ƒç”¨hybrid_predict.pyå¤±è´¥: {e}")
            return [], []
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for temp_file in [temp_fasta, temp_output]:
                if os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except:
                        pass
    
    def generate_10k_sequences_with_scores(self, 
                                         output_file: str = "generated_10k_with_hybrid_scores.fasta",
                                         batch_size: int = 50,
                                         base_seq_length: int = 50) -> None:
        """
        ç”Ÿæˆ10000æ¡åºåˆ—å¹¶ä½¿ç”¨hybrid_predict.pyè®¡ç®—å¾—åˆ†
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            batch_size: æ‰¹æ¬¡å¤§å°
            base_seq_length: åŸºç¡€åºåˆ—é•¿åº¦
        """
        logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆ10000æ¡åºåˆ—å¹¶ä½¿ç”¨hybrid_predict.pyè®¡ç®—å¾—åˆ†...")
        
        all_sequences = []
        all_probabilities = []
        all_predictions = []
        
        total_batches = (10000 + batch_size - 1) // batch_size
        
        with tqdm(total=10000, desc="ç”Ÿæˆå’Œé¢„æµ‹åºåˆ—") as pbar:
            for batch_idx in range(total_batches):
                # è®¡ç®—å½“å‰æ‰¹æ¬¡å¤§å°
                current_batch_size = min(batch_size, 10000 - len(all_sequences))
                
                if current_batch_size <= 0:
                    break
                
                # éšæœºåºåˆ—é•¿åº¦ï¼ˆåœ¨åˆç†èŒƒå›´å†…ï¼‰
                current_seq_length = np.random.randint(25, 65)
                
                # éšæœºæ¸©åº¦ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
                temperature = np.random.uniform(0.7, 1.2)
                
                try:
                    # ç”Ÿæˆåºåˆ—
                    sequences = self.generate_batch_sequences(
                        batch_size=current_batch_size,
                        seq_length=current_seq_length,
                        temperature=temperature
                    )
                    
                    if not sequences:
                        logger.warning(f"æ‰¹æ¬¡ {batch_idx} æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆåºåˆ—")
                        continue
                    
                    # é¢„æµ‹åºåˆ—
                    probabilities, predictions = self.predict_sequences_with_hybrid_script(sequences)
                    
                    if len(probabilities) != len(sequences):
                        logger.warning(f"æ‰¹æ¬¡ {batch_idx} é¢„æµ‹ç»“æœæ•°é‡ä¸åŒ¹é…: {len(probabilities)} vs {len(sequences)}")
                        # è¡¥é½ç¼ºå¤±çš„é¢„æµ‹ç»“æœ
                        while len(probabilities) < len(sequences):
                            probabilities.append(0.0)
                            predictions.append(0)
                    
                    # æ·»åŠ åˆ°æ€»ç»“æœ
                    all_sequences.extend(sequences)
                    all_probabilities.extend(probabilities)
                    all_predictions.extend(predictions)
                    
                    pbar.update(len(sequences))
                    
                    if len(all_sequences) >= 10000:
                        break
                        
                except Exception as e:
                    logger.warning(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    continue
        
        # ç¡®ä¿æ­£å¥½æœ‰10000æ¡åºåˆ—
        all_sequences = all_sequences[:10000]
        all_probabilities = all_probabilities[:10000]
        all_predictions = all_predictions[:10000]
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆå¹¶é¢„æµ‹ {len(all_sequences)} æ¡åºåˆ—")
        
        # ä¿å­˜åˆ°FASTAæ–‡ä»¶
        self.save_sequences_with_scores(
            sequences=all_sequences,
            probabilities=all_probabilities,
            predictions=all_predictions,
            output_file=output_file
        )
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(all_sequences, all_probabilities, all_predictions)
    
    def save_sequences_with_scores(self, sequences: List[str], probabilities: List[float],
                                 predictions: List[int], output_file: str):
        """ä¿å­˜åºåˆ—å’Œå¾—åˆ†åˆ°FASTAæ–‡ä»¶"""
        logger.info(f"ğŸ’¾ ä¿å­˜åºåˆ—åˆ° {output_file}...")
        
        records = []
        for i, (seq, prob, pred) in enumerate(zip(sequences, probabilities, predictions), 1):
            # åˆ›å»ºæè¿°ä¿¡æ¯
            label = "Anti-Gram-Negative" if pred == 1 else "Non-Anti-Gram-Negative"
            description = f"Generated_Seq_{i:05d} | Score: {prob:.4f} | Prediction: {label} | Length: {len(seq)}"
            
            # åˆ›å»ºSeqRecord
            record = SeqRecord(
                Seq(seq),
                id=f"Generated_Seq_{i:05d}",
                description=description
            )
            records.append(record)
        
        # å†™å…¥FASTAæ–‡ä»¶
        with open(output_file, 'w') as f:
            SeqIO.write(records, f, "fasta")
        
        logger.info(f"ğŸ‰ å®Œæˆï¼{len(records)}æ¡åºåˆ—å·²ä¿å­˜åˆ° {output_file}")
    
    def print_statistics(self, sequences: List[str], probabilities: List[float], predictions: List[int]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        lengths = [len(seq) for seq in sequences]
        
        logger.info("ğŸ“Š ç”Ÿæˆåºåˆ—ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"   åºåˆ—æ•°é‡: {len(sequences)}")
        logger.info(f"   å¹³å‡é•¿åº¦: {np.mean(lengths):.1f} Â± {np.std(lengths):.1f}")
        logger.info(f"   é•¿åº¦èŒƒå›´: {min(lengths)} - {max(lengths)}")
        logger.info(f"   å¹³å‡é¢„æµ‹å¾—åˆ†: {np.mean(probabilities):.3f} Â± {np.std(probabilities):.3f}")
        logger.info(f"   å¾—åˆ†èŒƒå›´: {min(probabilities):.3f} - {max(probabilities):.3f}")
        logger.info(f"   é¢„æµ‹ä¸ºæŠ—é©å…°æ°é˜´æ€§èŒè‚½: {sum(predictions)} ({sum(predictions)/len(predictions)*100:.1f}%)")
        logger.info(f"   é¢„æµ‹ä¸ºéæŠ—é©å…°æ°é˜´æ€§èŒè‚½: {len(predictions)-sum(predictions)} ({(len(predictions)-sum(predictions))/len(predictions)*100:.1f}%)")
        logger.info(f"   é«˜åˆ†åºåˆ— (>0.8): {sum(1 for p in probabilities if p > 0.8)}")
        logger.info(f"   ä¸­åˆ†åºåˆ— (0.5-0.8): {sum(1 for p in probabilities if 0.5 <= p <= 0.8)}")
        logger.info(f"   ä½åˆ†åºåˆ— (<0.5): {sum(1 for p in probabilities if p < 0.5)}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ç”Ÿæˆå™¨å’Œhybrid_predict.pyç”Ÿæˆ10000æ¡åºåˆ—å¹¶è®¡ç®—çœŸå®é¢„æµ‹å¾—åˆ†")
    parser.add_argument("--generator_checkpoint", type=str, required=True,
                       help="ç”Ÿæˆå™¨æ£€æŸ¥ç‚¹è·¯å¾„ (ä¾‹å¦‚: checkpoints_hybrid_650M/checkpoint_epoch_200.pt)")
    parser.add_argument("--hybrid_predict_script", type=str, default="hybrid_predict.py",
                       help="hybrid_predict.pyè„šæœ¬è·¯å¾„")
    parser.add_argument("--model_weights", type=str, default="model/best_weights.h5",
                       help="åˆ†ç±»å™¨æƒé‡è·¯å¾„")
    parser.add_argument("--scaler_path", type=str, default="model/scaler.pkl",
                       help="æ ‡å‡†åŒ–å™¨è·¯å¾„")
    parser.add_argument("--feature_names_file", type=str, default="data/feature_names.txt",
                       help="ç‰¹å¾åç§°æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", type=str, default="dual_4090",
                       help="é…ç½®åç§°")
    parser.add_argument("--output", type=str, default="generated_10k_with_hybrid_scores.fasta",
                       help="è¾“å‡ºFASTAæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=50,
                       help="æ‰¹æ¬¡å¤§å°")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–æ··åˆç”Ÿæˆå™¨
        generator = HybridSequenceGenerator(
            generator_checkpoint=args.generator_checkpoint,
            hybrid_predict_script=args.hybrid_predict_script,
            model_weights=args.model_weights,
            scaler_path=args.scaler_path,
            feature_names_file=args.feature_names_file,
            config_name=args.config
        )
        
        # æ£€æŸ¥ä¾èµ–æ–‡ä»¶
        generator.check_classifier_dependencies()
        
        # åŠ è½½ç”Ÿæˆå™¨
        generator.load_generator()
        
        # ç”Ÿæˆ10000æ¡åºåˆ—
        generator.generate_10k_sequences_with_scores(
            output_file=args.output,
            batch_size=args.batch_size
        )
        
        logger.info("ğŸŠ ä»»åŠ¡å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
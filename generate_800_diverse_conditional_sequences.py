#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é«˜è´¨é‡æŠ—èŒè‚½åºåˆ—ç”Ÿæˆå™¨ - 800æ¡åºåˆ—
ç»“åˆæ¡ä»¶D3PM + ESM-2å¼•å¯¼ + å¤šæ ·æ€§é‡‡æ ·é€»è¾‘
"""

import os
import sys
import torch
import random
import argparse
import logging
import subprocess
from datetime import datetime
import torch.nn.functional as F

# è§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜
project_root = os.path.abspath(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥å¿…è¦ç»„ä»¶
try:
    from enhanced_architecture.conditional_esm2_extractor import ConditionalESM2FeatureExtractor
    from gram_predictor.diffusion_models.conditional_d3pm import ConditionalD3PMUNet, ConditionalD3PMDiffusion, D3PMScheduler
    from gram_predictor.data_loader import tokens_to_sequence, AMINO_ACID_VOCAB
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'diverse_generation_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DiverseConditionalD3PMDiffusion(ConditionalD3PMDiffusion):
    """æ‰©å±•æ¡ä»¶D3PMï¼Œæ·»åŠ å¤šæ ·æ€§é‡‡æ ·åŠŸèƒ½"""
    
    @torch.no_grad()
    def diverse_sample(self,
                      batch_size: int,
                      seq_len: int,
                      condition_features: torch.Tensor = None,
                      guidance_scale: float = 5.0,
                      temperature: float = 1.0,
                      diversity_strength: float = 0.3) -> torch.Tensor:
        """
        å¤šæ ·æ€§æ„ŸçŸ¥çš„æ¡ä»¶é‡‡æ ·ï¼šç»“åˆCFGå’Œå¤šæ ·æ€§æ§åˆ¶
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            seq_len: åºåˆ—é•¿åº¦
            condition_features: æ¡ä»¶ç‰¹å¾ [batch_size, condition_dim]
            guidance_scale: æŒ‡å¯¼å¼ºåº¦
            temperature: é‡‡æ ·æ¸©åº¦
            diversity_strength: å¤šæ ·æ€§å¼ºåº¦ (0-1)
        """
        self.model.eval()
        
        # è®¾ç½®ç›®æ ‡æ°¨åŸºé…¸åˆ†å¸ƒï¼ˆåŸºäºè®­ç»ƒæ•°æ®ï¼‰
        target_distribution = {
            'K': 0.1136, 'G': 0.1045, 'L': 0.0897, 'R': 0.0888, 'A': 0.0719,
            'I': 0.0612, 'V': 0.0577, 'P': 0.0569, 'S': 0.0500, 'C': 0.0459,
            'F': 0.0436, 'T': 0.0362, 'N': 0.0320, 'Q': 0.0248, 'D': 0.0247,
            'E': 0.0238, 'W': 0.0216, 'Y': 0.0209, 'H': 0.0198, 'M': 0.0122
        }
        
        # è½¬æ¢ä¸ºtokenåˆ†å¸ƒ
        target_token_probs = torch.zeros(self.scheduler.vocab_size, device=self.device)
        for aa, prob in target_distribution.items():
            if aa in AMINO_ACID_VOCAB:
                token_id = AMINO_ACID_VOCAB[aa]
                target_token_probs[token_id] = prob
        target_token_probs = target_token_probs / target_token_probs.sum()
        
        # ä»éšæœºæ°¨åŸºé…¸å¼€å§‹ï¼ˆä¸åŒ…å«PAD tokenï¼‰
        x = torch.randint(1, self.scheduler.vocab_size, (batch_size, seq_len),
                          device=self.device)
        
        # é€†å‘æ‰©æ•£è¿‡ç¨‹
        timesteps = torch.linspace(self.scheduler.num_timesteps - 1, 0,
                                   self.scheduler.num_timesteps, dtype=torch.long,
                                   device=self.device)
        
        for t in timesteps:
            t_batch = t.repeat(batch_size)
            
            # Classifier-Free Guidance (CFG)
            # 1. é¢„æµ‹æœ‰æ¡ä»¶çš„logits
            logits_cond = self.model(x, t_batch, condition_features)
            
            # 2. é¢„æµ‹æ— æ¡ä»¶çš„logits
            logits_uncond = self.model(x, t_batch, None)
            
            # 3. ç»“åˆæœ‰æ¡ä»¶å’Œæ— æ¡ä»¶çš„é¢„æµ‹
            guided_logits = logits_uncond + guidance_scale * (logits_cond - logits_uncond)
            
            # å±è”½PAD tokençš„æ¦‚ç‡ï¼Œé¿å…ç”ŸæˆPAD
            guided_logits[:, :, 0] = float('-inf')
            
            # åº”ç”¨å¤šæ ·æ€§è°ƒæ•´
            if diversity_strength > 0:
                # è®¡ç®—å½“å‰åºåˆ—çš„æ°¨åŸºé…¸åˆ†å¸ƒ
                diversity_adjustment = torch.zeros_like(guided_logits)
                for b in range(batch_size):
                    current_counts = torch.bincount(x[b], minlength=self.scheduler.vocab_size).float()
                    current_dist = current_counts / (current_counts.sum() + 1e-8)
                    
                    for pos in range(seq_len):
                        # æƒ©ç½šè¿‡åº¦å‡ºç°çš„æ°¨åŸºé…¸
                        overpresented = current_dist > target_token_probs * 1.5
                        diversity_adjustment[b, pos, overpresented] = -diversity_strength * 3
                        
                        # å¥–åŠ±ä¸è¶³çš„æ°¨åŸºé…¸
                        underpresented = current_dist < target_token_probs * 0.5
                        diversity_adjustment[b, pos, underpresented] = diversity_strength * 2
                
                guided_logits = guided_logits + diversity_adjustment
            
            # åº”ç”¨æ¸©åº¦
            scaled_logits = guided_logits / temperature
            
            # ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·
            probs = F.softmax(scaled_logits, dim=-1)
            
            # åœ¨æœ€åä¸€æ­¥ä½¿ç”¨argmaxä»¥è·å¾—ç¡®å®šæ€§ç»“æœï¼Œå¦åˆ™è¿›è¡Œå¤šé¡¹å¼é‡‡æ ·
            if t == 0:
                x = torch.argmax(probs, dim=-1)
            else:
                x = torch.multinomial(probs.view(-1, self.scheduler.vocab_size),
                                      num_samples=1).view(batch_size, seq_len)
        
        self.model.train()
        return x

class DiverseConditionalSequenceGenerator:
    """å¤šæ ·æ€§æ¡ä»¶åºåˆ—ç”Ÿæˆå™¨"""
    
    def __init__(self, checkpoint_path: str = None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # è‡ªåŠ¨æ£€æµ‹æ£€æŸ¥ç‚¹è·¯å¾„
        if checkpoint_path is None:
            checkpoint_path = self.find_best_checkpoint()
        
        self.checkpoint_path = checkpoint_path
        self.diffusion_model = None
        self.feature_extractor = None
        
        # æ¨¡å‹é…ç½®
        self.model_config = {
            "esm_model": "facebook/esm2_t33_650M_UR50D",
            "condition_dim": 512,
            "hidden_dim": 512,
            "num_layers": 8,
            "num_timesteps": 1000,
            "max_seq_len": 100,
        }
        
        # ç”Ÿæˆå‚æ•° - æ‚¨è¦æ±‚çš„diverseé‡‡æ ·å‚æ•°
        self.generation_params = {
            "guidance_scale": 5.0,        # CFGå¼•å¯¼å¼ºåº¦
            "temperature": 0.8,           # é‡‡æ ·æ¸©åº¦
            "diversity_strength": 1.2,    # å¤šæ ·æ€§å¼ºåº¦ (ä¸generate_300_sequences.pyä¸€è‡´)
            "min_len": 20,
            "max_len": 50,
            "num_references": 10,
        }
        
    def find_best_checkpoint(self):
        """è‡ªåŠ¨æŸ¥æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹"""
        possible_paths = [
            "enhanced_architecture/output/checkpoints/best.pt",
            "enhanced_architecture/output/checkpoints/latest.pt",
            "gram_predictor/models/best.pt",
            "checkpoints/best.pt",
            "output/checkpoints/best.pt"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                logger.info(f"âœ… æ‰¾åˆ°æ£€æŸ¥ç‚¹: {path}")
                return path
        
        # æœç´¢æ‰€æœ‰.ptæ–‡ä»¶
        for root, dirs, files in os.walk("."):
            for file in files:
                if file.endswith(".pt") and ("best" in file or "checkpoint" in file):
                    full_path = os.path.join(root, file)
                    logger.info(f"ğŸ” å‘ç°æ£€æŸ¥ç‚¹: {full_path}")
                    return full_path
        
        raise FileNotFoundError("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼")
    
    def load_models(self):
        """åŠ è½½æ¨¡å‹ç»„ä»¶"""
        logger.info(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {self.checkpoint_path}")
        
        if not os.path.exists(self.checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {self.checkpoint_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device, weights_only=False)
        logger.info(f"ğŸ“Š æ£€æŸ¥ç‚¹ä¿¡æ¯: Epoch {checkpoint.get('epoch', 'Unknown')}, Loss {checkpoint.get('best_val_loss', 'Unknown')}")
        
        # 1. åˆå§‹åŒ–ESM-2ç‰¹å¾æå–å™¨
        self.feature_extractor = ConditionalESM2FeatureExtractor(
            model_name=self.model_config["esm_model"],
            condition_dim=self.model_config["condition_dim"]
        ).to(self.device)
        
        # 2. åˆå§‹åŒ–æ‰©å±•çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆå¸¦å¤šæ ·æ€§é‡‡æ ·ï¼‰
        unet = ConditionalD3PMUNet(
            hidden_dim=self.model_config["hidden_dim"],
            num_layers=self.model_config["num_layers"],
            condition_dim=self.model_config["condition_dim"],
            max_seq_len=self.model_config["max_seq_len"]
        )
        
        scheduler = D3PMScheduler(num_timesteps=self.model_config["num_timesteps"])
        self.diffusion_model = DiverseConditionalD3PMDiffusion(unet, scheduler, self.device)
        
        # 3. åŠ è½½æƒé‡ - æ™ºèƒ½æ£€æµ‹é”®å
        try:
            # æ£€æµ‹å¯èƒ½çš„é”®å
            possible_keys = ['model_state_dict', 'diffusion_model_state_dict', 'unet_state_dict', 'state_dict']
            model_state_dict = None
            
            logger.info("æ£€æŸ¥ç‚¹åŒ…å«çš„é”®:")
            for key in checkpoint.keys():
                logger.info(f"  {key}")
            
            # å°è¯•æ‰¾åˆ°æ­£ç¡®çš„æ¨¡å‹çŠ¶æ€å­—å…¸
            for key in possible_keys:
                if key in checkpoint:
                    model_state_dict = checkpoint[key]
                    logger.info(f"âœ… ä½¿ç”¨é”®å: {key}")
                    break
            
            if model_state_dict is None:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†é”®åï¼Œå°è¯•ç›´æ¥ä½¿ç”¨æ£€æŸ¥ç‚¹
                if hasattr(checkpoint, 'state_dict'):
                    model_state_dict = checkpoint.state_dict()
                    logger.info("âœ… ä½¿ç”¨ checkpoint.state_dict()")
                else:
                    raise KeyError(f"æœªæ‰¾åˆ°æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚å¯ç”¨é”®: {list(checkpoint.keys())}")
            
            # å¤„ç†DataParallelåŒ…è£…
            if list(model_state_dict.keys())[0].startswith('module.'):
                from collections import OrderedDict
                new_state_dict = OrderedDict()
                for k, v in model_state_dict.items():
                    name = k[7:]  # ç§»é™¤'module.'å‰ç¼€
                    new_state_dict[name] = v
                model_state_dict = new_state_dict
                logger.info("âœ… ç§»é™¤äº†DataParallelå‰ç¼€")
            
            self.diffusion_model.model.load_state_dict(model_state_dict)
            logger.info("âœ… æ‰©æ•£æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
            logger.error(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ç»“æ„: {list(checkpoint.keys())}")
            raise
        
        logger.info("ğŸ¯ æ‰€æœ‰æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆ")
    
    def get_reference_sequences(self, num_refs: int):
        """è·å–å‚è€ƒåºåˆ—"""
        reference_files = [
            "enhanced_architecture/gram_neg_only.txt",
            "enhanced_architecture/gram_both.txt"
        ]
        
        all_seqs = []
        for file_path in reference_files:
            if os.path.exists(file_path):
                with open(file_path, 'r') as f:
                    seqs = [line.strip() for line in f if line.strip() and len(line.strip()) >= 15]
                    all_seqs.extend(seqs)
        
        if not all_seqs:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å‚è€ƒåºåˆ—æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤åºåˆ—")
            all_seqs = [
                "KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQATQIAK",
                "GIGKFLHSAKKFGKAFVGEIMNS",
                "KWKLFKKIGAVLKVLTTGLPALIS",
                "FLGALFKALKAA"
            ] * 3  # é‡å¤ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„åºåˆ—
        
        if len(all_seqs) < num_refs:
            all_seqs = all_seqs * ((num_refs // len(all_seqs)) + 1)
        
        selected = random.sample(all_seqs, min(num_refs, len(all_seqs)))
        logger.info(f"ğŸ“‹ é€‰æ‹©äº† {len(selected)} æ¡å‚è€ƒåºåˆ—")
        return selected
    
    def generate_sequences(self, num_sequences: int = 800):
        """ç”Ÿæˆåºåˆ— - ä½¿ç”¨å¤šæ ·æ€§æ¡ä»¶é‡‡æ ·"""
        logger.info(f"ğŸ¯ å¼€å§‹ç”Ÿæˆ {num_sequences} æ¡åºåˆ— (å¤šæ ·æ€§æ¡ä»¶é‡‡æ ·)")
        logger.info(f"ğŸ“Š å‚æ•°: guidance_scale={self.generation_params['guidance_scale']}, "
                   f"temperature={self.generation_params['temperature']}, "
                   f"diversity_strength={self.generation_params['diversity_strength']}")
        
        # åˆ†æ‰¹ç”Ÿæˆ
        batch_size = 50
        all_sequences = []
        
        # è·å–å‚è€ƒåºåˆ—å’Œæ¡ä»¶ç‰¹å¾
        ref_sequences = self.get_reference_sequences(self.generation_params["num_references"])
        logger.info("å‚è€ƒåºåˆ—ç¤ºä¾‹: " + ", ".join(ref_sequences[:3]) + "...")
        
        with torch.no_grad():
            condition_features = self.feature_extractor.extract_condition_features(ref_sequences)
            final_condition = condition_features.mean(dim=0).unsqueeze(0)
        
        num_batches = (num_sequences + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            current_batch_size = min(batch_size, num_sequences - batch_idx * batch_size)
            logger.info(f"ğŸ”„ ç”Ÿæˆæ‰¹æ¬¡ {batch_idx + 1}/{num_batches} ({current_batch_size} æ¡åºåˆ—)")
            
            batch_condition = final_condition.repeat(current_batch_size, 1)
            
            # éšæœºåºåˆ—é•¿åº¦
            target_lengths = [random.randint(self.generation_params["min_len"], 
                                           self.generation_params["max_len"]) 
                            for _ in range(current_batch_size)]
            max_length = max(target_lengths)
            
            # ä½¿ç”¨å¤šæ ·æ€§æ¡ä»¶é‡‡æ ·
            with torch.no_grad():
                generated_tokens = self.diffusion_model.diverse_sample(
                    batch_size=current_batch_size,
                    seq_len=max_length,
                    condition_features=batch_condition,
                    guidance_scale=self.generation_params["guidance_scale"],
                    temperature=self.generation_params["temperature"],
                    diversity_strength=self.generation_params["diversity_strength"]
                )
            
            # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
            for i, (tokens, target_len) in enumerate(zip(generated_tokens, target_lengths)):
                full_sequence = tokens_to_sequence(tokens.cpu().numpy())
                truncated_sequence = full_sequence[:target_len]
                
                if len(truncated_sequence) >= 10 and truncated_sequence.count('X') == 0:
                    all_sequences.append({
                        'id': f'DiverseConditional_Seq_{len(all_sequences) + 1:04d}',
                        'sequence': truncated_sequence,
                        'length': len(truncated_sequence),
                        'batch': batch_idx + 1
                    })
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(all_sequences)} æ¡åºåˆ—")
        return all_sequences
    
    def save_sequences(self, sequences, output_prefix="diverse_conditional_800_sequences"):
        """ä¿å­˜ç”Ÿæˆçš„åºåˆ—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ä¸ºFASTAæ ¼å¼
        fasta_file = f"{output_prefix}_{timestamp}.fasta"
        with open(fasta_file, 'w') as f:
            for seq_data in sequences:
                f.write(f">{seq_data['id']} | Length={seq_data['length']} | "
                       f"CFG={self.generation_params['guidance_scale']} | "
                       f"T={self.generation_params['temperature']} | "
                       f"D={self.generation_params['diversity_strength']}\n")
                f.write(f"{seq_data['sequence']}\n")
        
        logger.info(f"ğŸ’¾ åºåˆ—å·²ä¿å­˜åˆ°: {fasta_file}")
        return fasta_file

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤šæ ·æ€§æ¡ä»¶æŠ—èŒè‚½åºåˆ—ç”Ÿæˆå™¨")
    parser.add_argument("--checkpoint", type=str, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--num_sequences", type=int, default=800, help="ç”Ÿæˆåºåˆ—æ•°é‡")
    parser.add_argument("--guidance_scale", type=float, default=5.0, help="CFGå¼•å¯¼å¼ºåº¦")
    parser.add_argument("--temperature", type=float, default=0.8, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--diversity_strength", type=float, default=1.2, help="å¤šæ ·æ€§å¼ºåº¦")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = DiverseConditionalSequenceGenerator(args.checkpoint)
        
        # æ›´æ–°å‚æ•°
        generator.generation_params.update({
            "guidance_scale": args.guidance_scale,
            "temperature": args.temperature,
            "diversity_strength": args.diversity_strength
        })
        
        # åŠ è½½æ¨¡å‹
        generator.load_models()
        
        # ç”Ÿæˆåºåˆ—
        sequences = generator.generate_sequences(args.num_sequences)
        
        if not sequences:
            logger.error("âŒ æœªç”Ÿæˆä»»ä½•åºåˆ—")
            return
        
        # ä¿å­˜åºåˆ—
        fasta_file = generator.save_sequences(sequences)
        
        # ç»Ÿè®¡æŠ¥å‘Š
        lengths = [seq['length'] for seq in sequences]
        logger.info("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡:")
        logger.info(f"   æ€»åºåˆ—æ•°: {len(sequences)}")
        logger.info(f"   å¹³å‡é•¿åº¦: {sum(lengths)/len(lengths):.1f}")
        logger.info(f"   é•¿åº¦èŒƒå›´: {min(lengths)}-{max(lengths)}")
        logger.info(f"   CFGå¼•å¯¼å¼ºåº¦: {generator.generation_params['guidance_scale']}")
        logger.info(f"   é‡‡æ ·æ¸©åº¦: {generator.generation_params['temperature']}")
        logger.info(f"   å¤šæ ·æ€§å¼ºåº¦: {generator.generation_params['diversity_strength']}")
        
        logger.info("ğŸ‰ å¤šæ ·æ€§æ¡ä»¶ç”Ÿæˆå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
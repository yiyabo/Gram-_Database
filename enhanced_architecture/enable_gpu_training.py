#!/usr/bin/env python3
"""
å¯ç”¨GPUè®­ç»ƒçš„é…ç½®è„šæœ¬
"""

import torch
import os

def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    print("ğŸ” æ£€æŸ¥GPUå¯ç”¨æ€§...")
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1024**3
        
        print(f"âœ… CUDAå¯ç”¨")
        print(f"  â€¢ GPUæ•°é‡: {gpu_count}")
        print(f"  â€¢ å½“å‰GPU: {gpu_name}")
        print(f"  â€¢ æ˜¾å­˜å¤§å°: {gpu_memory:.1f} GB")
        
        return True, "cuda"
    
    # æ£€æŸ¥MPS (Apple Silicon)
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print(f"âœ… MPSå¯ç”¨ (Apple Silicon)")
        print(f"  â€¢ è®¾å¤‡: Apple Silicon GPU")
        return True, "mps"
    
    else:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        return False, "cpu"

def enable_gpu_training():
    """å¯ç”¨GPUè®­ç»ƒ"""
    gpu_available, device_type = check_gpu_availability()
    
    if not gpu_available:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„GPUï¼Œä¿æŒCPUè®­ç»ƒé…ç½®")
        return False
    
    # è¯»å–ä¸»è®­ç»ƒå™¨æ–‡ä»¶
    trainer_file = "main_trainer.py"
    
    if not os.path.exists(trainer_file):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {trainer_file}")
        return False
    
    with open(trainer_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å¯ç”¨GPU
    if 'torch.device("cuda"' in content or 'torch.device("mps"' in content:
        print("âœ… GPUè®­ç»ƒå·²ç»å¯ç”¨")
        return True
    
    # æ›¿æ¢è®¾å¤‡é…ç½®
    old_device_config = '''        # å¼ºåˆ¶ä½¿ç”¨CPUä»¥é¿å…MPSå…¼å®¹æ€§é—®é¢˜
        self.device = torch.device("cpu")
        # å¦‚æœåœ¨æœåŠ¡å™¨ä¸Šï¼Œå¯ä»¥æ”¹ä¸ºï¼š
        # self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")'''
    
    if device_type == "cuda":
        new_device_config = '''        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡ (GPUä¼˜å…ˆ)
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            print(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {torch.cuda.get_device_name()}")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")'''
    
    elif device_type == "mps":
        new_device_config = '''        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡ (Apple Silicon GPUä¼˜å…ˆ)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("ğŸš€ ä½¿ç”¨Apple Silicon GPUè®­ç»ƒ")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print(f"ğŸš€ ä½¿ç”¨CUDA GPUè®­ç»ƒ: {torch.cuda.get_device_name()}")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")'''
    
    # æ‰§è¡Œæ›¿æ¢
    if old_device_config in content:
        new_content = content.replace(old_device_config, new_device_config)
        
        # å¤‡ä»½åŸæ–‡ä»¶
        backup_file = f"{trainer_file}.backup"
        with open(backup_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"ğŸ“ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")
        
        # å†™å…¥æ–°é…ç½®
        with open(trainer_file, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"âœ… å·²å¯ç”¨{device_type.upper()}è®­ç»ƒ")
        print("ğŸ’¡ å¦‚éœ€æ¢å¤CPUè®­ç»ƒï¼Œè¯·è¿è¡Œ: python disable_gpu_training.py")
        
        return True
    else:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„æœŸçš„è®¾å¤‡é…ç½®ä»£ç ï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹")
        return False

def disable_gpu_training():
    """ç¦ç”¨GPUè®­ç»ƒï¼Œæ¢å¤CPUé…ç½®"""
    trainer_file = "main_trainer.py"
    backup_file = f"{trainer_file}.backup"
    
    if os.path.exists(backup_file):
        with open(backup_file, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        with open(trainer_file, 'w', encoding='utf-8') as f:
            f.write(original_content)
        
        print("âœ… å·²æ¢å¤CPUè®­ç»ƒé…ç½®")
        os.remove(backup_file)
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤å¤‡ä»½æ–‡ä»¶: {backup_file}")
        return True
    else:
        print("âŒ æ‰¾ä¸åˆ°å¤‡ä»½æ–‡ä»¶ï¼Œæ— æ³•è‡ªåŠ¨æ¢å¤")
        return False

def get_recommended_batch_size(device_type, gpu_memory_gb=None):
    """æ ¹æ®è®¾å¤‡ç±»å‹æ¨èæ‰¹æ¬¡å¤§å°"""
    if device_type == "cpu":
        return 8, "CPUè®­ç»ƒå»ºè®®ä½¿ç”¨è¾ƒå°æ‰¹æ¬¡"
    
    elif device_type == "mps":
        return 16, "Apple Silicon GPUå»ºè®®æ‰¹æ¬¡å¤§å°"
    
    elif device_type == "cuda":
        if gpu_memory_gb is None:
            return 32, "CUDA GPUé»˜è®¤æ‰¹æ¬¡å¤§å°"
        elif gpu_memory_gb >= 24:
            return 64, "å¤§æ˜¾å­˜GPUå¯ä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡"
        elif gpu_memory_gb >= 12:
            return 32, "ä¸­ç­‰æ˜¾å­˜GPUæ¨èæ‰¹æ¬¡"
        elif gpu_memory_gb >= 8:
            return 16, "è¾ƒå°æ˜¾å­˜GPUå»ºè®®æ‰¹æ¬¡"
        else:
            return 8, "æ˜¾å­˜ä¸è¶³ï¼Œä½¿ç”¨å°æ‰¹æ¬¡"
    
    return 16, "é»˜è®¤æ‰¹æ¬¡å¤§å°"

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="GPUè®­ç»ƒé…ç½®å·¥å…·")
    parser.add_argument("--enable", action="store_true", help="å¯ç”¨GPUè®­ç»ƒ")
    parser.add_argument("--disable", action="store_true", help="ç¦ç”¨GPUè®­ç»ƒ")
    parser.add_argument("--check", action="store_true", help="ä»…æ£€æŸ¥GPUå¯ç”¨æ€§")
    
    args = parser.parse_args()
    
    if args.disable:
        disable_gpu_training()
    elif args.enable:
        enable_gpu_training()
    elif args.check:
        gpu_available, device_type = check_gpu_availability()
        if gpu_available:
            if device_type == "cuda":
                current_device = torch.cuda.current_device()
                gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1024**3
                batch_size, reason = get_recommended_batch_size(device_type, gpu_memory)
            else:
                batch_size, reason = get_recommended_batch_size(device_type)
            
            print(f"\nğŸ’¡ æ¨èé…ç½®:")
            print(f"  â€¢ æ¨èæ‰¹æ¬¡å¤§å°: {batch_size} ({reason})")
            print(f"  â€¢ è®¾å¤‡ç±»å‹: {device_type}")
    else:
        # é»˜è®¤è¡Œä¸ºï¼šæ£€æŸ¥å¹¶è¯¢é—®æ˜¯å¦å¯ç”¨
        gpu_available, device_type = check_gpu_availability()
        
        if gpu_available:
            response = input(f"\nğŸ¤” æ£€æµ‹åˆ°{device_type.upper()}å¯ç”¨ï¼Œæ˜¯å¦å¯ç”¨GPUè®­ç»ƒ? (y/n): ").lower().strip()
            if response in ['y', 'yes', 'æ˜¯']:
                enable_gpu_training()
            else:
                print("ä¿æŒå½“å‰CPUè®­ç»ƒé…ç½®")
        else:
            print("å°†ç»§ç»­ä½¿ç”¨CPUè®­ç»ƒ")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
å¿«é€Ÿè¯Šæ–­ç”Ÿæˆé—®é¢˜
"""

import torch
import time
from data_loader import sequence_to_tokens, tokens_to_sequence, AMINO_ACID_VOCAB
from diffusion_models.d3pm_diffusion import D3PMDiffusion, D3PMScheduler, D3PMUNet

def quick_generation_test():
    """å¿«é€Ÿæµ‹è¯•ç”ŸæˆåŠŸèƒ½"""
    print("ğŸ” å¿«é€Ÿç”Ÿæˆæµ‹è¯•å¼€å§‹...")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    vocab_size = len(AMINO_ACID_VOCAB)
    scheduler = D3PMScheduler(num_timesteps=50)  # å‡å°‘timestepsåŠ é€Ÿæµ‹è¯•
    
    unet = D3PMUNet(
        vocab_size=vocab_size,
        hidden_dim=256,  # å‡å°hidden_dim
        num_layers=4,    # å‡å°‘å±‚æ•°
        num_heads=8,
        max_seq_len=30   # æ­£ç¡®çš„å‚æ•°å
    )
    
    diffusion = D3PMDiffusion(
        model=unet,
        scheduler=scheduler,
        vocab_size=vocab_size
    ).to(device)
    
    print("âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # æµ‹è¯•1: ç®€å•é‡‡æ ·
    print("\nğŸ§ª æµ‹è¯•1: ç®€å•é‡‡æ ·...")
    start_time = time.time()
    
    try:
        generated_tokens = diffusion.sample(
            batch_size=1,
            seq_len=20,  # æ›´çŸ­çš„åºåˆ—
            device=device
        )
        
        sequence = tokens_to_sequence(generated_tokens[0])
        elapsed = time.time() - start_time
        
        print(f"âœ“ ç®€å•é‡‡æ ·æˆåŠŸ ({elapsed:.2f}s)")
        print(f"  ç”Ÿæˆåºåˆ—: {sequence}")
        
    except Exception as e:
        print(f"âŒ ç®€å•é‡‡æ ·å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•2: å¤šæ ·æ€§é‡‡æ ·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    print("\nğŸ§ª æµ‹è¯•2: æ£€æŸ¥å¤šæ ·æ€§é‡‡æ ·æ–¹æ³•...")
    
    if hasattr(diffusion, 'diverse_sample'):
        print("âœ“ æ‰¾åˆ° diverse_sample æ–¹æ³•")
        
        try:
            start_time = time.time()
            generated_tokens = diffusion.diverse_sample(
                batch_size=1,
                seq_len=20,
                diversity_strength=0.3,
                temperature=1.0
            )
            
            sequence = tokens_to_sequence(generated_tokens[0])
            elapsed = time.time() - start_time
            
            print(f"âœ“ å¤šæ ·æ€§é‡‡æ ·æˆåŠŸ ({elapsed:.2f}s)")
            print(f"  ç”Ÿæˆåºåˆ—: {sequence}")
            
        except Exception as e:
            print(f"âŒ å¤šæ ·æ€§é‡‡æ ·å¤±è´¥: {e}")
            print(f"  é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return False
    else:
        print("âŒ æœªæ‰¾åˆ° diverse_sample æ–¹æ³•")
        return False
    
    # æµ‹è¯•3: æ‰¹é‡ç”Ÿæˆ
    print("\nğŸ§ª æµ‹è¯•3: æ‰¹é‡ç”Ÿæˆ...")
    
    try:
        start_time = time.time()
        generated_tokens = diffusion.sample(
            batch_size=5,
            seq_len=15,
            device=device
        )
        
        sequences = [tokens_to_sequence(tokens) for tokens in generated_tokens]
        elapsed = time.time() - start_time
        
        print(f"âœ“ æ‰¹é‡ç”ŸæˆæˆåŠŸ ({elapsed:.2f}s)")
        print(f"  ç”Ÿæˆ {len(sequences)} æ¡åºåˆ—")
        for i, seq in enumerate(sequences):
            print(f"    {i+1}: {seq}")
            
    except Exception as e:
        print(f"âŒ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
        return False
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
    return True

def test_generation_speed():
    """æµ‹è¯•ç”Ÿæˆé€Ÿåº¦"""
    print("\nâ±ï¸ ç”Ÿæˆé€Ÿåº¦æµ‹è¯•...")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºç®€åŒ–æ¨¡å‹
    vocab_size = len(AMINO_ACID_VOCAB)
    scheduler = D3PMScheduler(num_timesteps=20)  # éå¸¸å°‘çš„steps
    
    unet = D3PMUNet(
        vocab_size=vocab_size,
        hidden_dim=128,  # å¾ˆå°çš„hidden_dim
        num_layers=2,    # å¾ˆå°‘çš„å±‚æ•°
        num_heads=4,
        max_seq_len=30   # æ­£ç¡®çš„å‚æ•°å
    )
    
    diffusion = D3PMDiffusion(
        model=unet,
        scheduler=scheduler,
        vocab_size=vocab_size
    ).to(device)
    
    # æµ‹è¯•ä¸åŒbatch_sizeçš„é€Ÿåº¦
    for batch_size in [1, 5, 10]:
        start_time = time.time()
        
        try:
            generated_tokens = diffusion.sample(
                batch_size=batch_size,
                seq_len=20,
                device=device
            )
            
            elapsed = time.time() - start_time
            per_sequence = elapsed / batch_size
            
            print(f"  Batch {batch_size}: {elapsed:.2f}s æ€»è®¡, {per_sequence:.2f}s/åºåˆ—")
            
        except Exception as e:
            print(f"  Batch {batch_size}: å¤±è´¥ - {e}")

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ ç”ŸæˆåŠŸèƒ½è¯Šæ–­æµ‹è¯•")
    print("=" * 60)
    
    # è¿è¡Œå¿«é€Ÿæµ‹è¯•
    success = quick_generation_test()
    
    if success:
        # è¿è¡Œé€Ÿåº¦æµ‹è¯•
        test_generation_speed()
    else:
        print("\nâŒ åŸºç¡€æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤æ¨¡å‹")

#!/usr/bin/env python3
"""
æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·å™¨
åŸºäºå½“å‰å·²æœ‰çš„è‰¯å¥½ç»“æœï¼Œè¿›è¡Œæ›´æ¸©å’Œçš„ä¼˜åŒ–
"""

import torch
import torch.nn.functional as F
import numpy as np
from collections import Counter
from typing import Dict, List, Tuple, Optional
from data_loader import AMINO_ACID_VOCAB, VOCAB_TO_AA, sequence_to_tokens, tokens_to_sequence
from diffusion_models.d3pm_diffusion import D3PMDiffusion, D3PMScheduler, D3PMUNet

class ImprovedDiversitySampler:
    """æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·å™¨ï¼Œæ›´æ¸©å’Œçš„è°ƒæ•´ç­–ç•¥"""
    
    def __init__(self):
        # åŸºäºçœŸå®è®­ç»ƒæ•°æ®çš„ç›®æ ‡åˆ†å¸ƒ
        self.target_distribution = {
            'K': 0.1136, 'G': 0.1045, 'L': 0.0897, 'R': 0.0888, 'A': 0.0719,
            'I': 0.0612, 'V': 0.0577, 'P': 0.0569, 'S': 0.0500, 'C': 0.0459,
            'F': 0.0436, 'T': 0.0362, 'N': 0.0320, 'Q': 0.0248, 'D': 0.0247,
            'E': 0.0238, 'W': 0.0216, 'Y': 0.0209, 'H': 0.0198, 'M': 0.0122
        }
        
        # è½¬æ¢ä¸ºtokenæ¦‚ç‡
        self.target_token_probs = torch.zeros(len(AMINO_ACID_VOCAB))
        for aa, prob in self.target_distribution.items():
            if aa in AMINO_ACID_VOCAB:
                token_id = AMINO_ACID_VOCAB[aa]
                self.target_token_probs[token_id] = prob
        
        print("âœ“ æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def gentle_diverse_sample(self, diffusion: D3PMDiffusion, batch_size: int, seq_len: int,
                             esm_features: Optional[torch.Tensor] = None,
                             num_inference_steps: Optional[int] = None,
                             diversity_strength: float = 0.1,  # æ›´æ¸©å’Œçš„å¼ºåº¦
                             temperature: float = 1.1,  # ç•¥é«˜çš„æ¸©åº¦
                             anti_repeat_strength: float = 0.2) -> torch.Tensor:
        """
        æ¸©å’Œçš„å¤šæ ·æ€§é‡‡æ ·
        
        Args:
            diffusion: æ‰©æ•£æ¨¡å‹
            batch_size: æ‰¹æ¬¡å¤§å°
            seq_len: åºåˆ—é•¿åº¦
            esm_features: ESMç‰¹å¾
            num_inference_steps: æ¨ç†æ­¥æ•°
            diversity_strength: å¤šæ ·æ€§å¼ºåº¦ï¼ˆé™ä½åˆ°0.1ï¼‰
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆç•¥å¾®æé«˜ï¼‰
            anti_repeat_strength: é˜²é‡å¤å¼ºåº¦
        """
        if num_inference_steps is None:
            num_inference_steps = diffusion.scheduler.num_timesteps
        
        device = diffusion.device
        target_probs = self.target_token_probs.to(device)
        
        # ä»éšæœºæ°¨åŸºé…¸å¼€å§‹
        x = torch.randint(1, diffusion.scheduler.vocab_size, (batch_size, seq_len), device=device)
        
        # é€†å‘æ‰©æ•£è¿‡ç¨‹
        timesteps = torch.linspace(diffusion.scheduler.num_timesteps - 1, 0, 
                                 num_inference_steps, dtype=torch.long, device=device)
        
        for i, t in enumerate(timesteps):
            t_batch = t.repeat(batch_size)
            predicted_logits = diffusion.model(x, t_batch, esm_features)
            
            # å±è”½PAD token
            predicted_logits[:, :, 0] = float('-inf')
            
            if i < len(timesteps) - 1:
                # æ¸©å’Œçš„å¤šæ ·æ€§è°ƒæ•´
                adjusted_logits = self._apply_gentle_diversity_adjustment(
                    predicted_logits, x, target_probs, diversity_strength, anti_repeat_strength
                )
                
                # æ¸©åº¦ç¼©æ”¾
                scaled_logits = adjusted_logits / temperature
                probs = F.softmax(scaled_logits, dim=-1)
                
                # é‡‡æ ·
                x = torch.multinomial(probs.view(-1, diffusion.scheduler.vocab_size), 
                                    num_samples=1).view(batch_size, seq_len)
            else:
                # æœ€åä¸€æ­¥ä½¿ç”¨argmax
                x = torch.argmax(predicted_logits, dim=-1)
        
        return x
    
    def _apply_gentle_diversity_adjustment(self, logits: torch.Tensor, current_x: torch.Tensor,
                                         target_probs: torch.Tensor, diversity_strength: float,
                                         anti_repeat_strength: float) -> torch.Tensor:
        """åº”ç”¨æ¸©å’Œçš„å¤šæ ·æ€§è°ƒæ•´"""
        batch_size, seq_len, vocab_size = logits.shape
        device = logits.device
        
        adjustment = torch.zeros_like(logits)
        
        for b in range(batch_size):
            # è®¡ç®—å½“å‰åºåˆ—çš„åˆ†å¸ƒ
            current_counts = torch.bincount(current_x[b], minlength=vocab_size).float()
            current_dist = current_counts / (current_counts.sum() + 1e-8)
            
            # 1. æ¸©å’Œçš„å…¨å±€å¤šæ ·æ€§è°ƒæ•´
            for pos in range(seq_len):
                # åªå¯¹ä¸¥é‡åç¦»ç›®æ ‡çš„æ°¨åŸºé…¸è¿›è¡Œè°ƒæ•´
                deviation = current_dist - target_probs
                
                # å¯¹ä¸¥é‡è¿‡å¤šçš„æ°¨åŸºé…¸è¿›è¡Œæ¸©å’Œæƒ©ç½š
                severely_overpresented = deviation > 0.05  # è¶…è¿‡5%æ‰è°ƒæ•´
                adjustment[b, pos, severely_overpresented] = -diversity_strength * deviation[severely_overpresented]
                
                # å¯¹ä¸¥é‡ä¸è¶³çš„æ°¨åŸºé…¸è¿›è¡Œæ¸©å’Œå¥–åŠ±
                severely_underpresented = deviation < -0.03  # å°‘äº3%æ‰è°ƒæ•´
                adjustment[b, pos, severely_underpresented] = -diversity_strength * deviation[severely_underpresented] * 0.5
            
            # 2. é˜²é‡å¤è°ƒæ•´ï¼ˆå±€éƒ¨æ¨¡å¼ï¼‰
            if anti_repeat_strength > 0:
                seq = current_x[b]
                for pos in range(seq_len):
                    # æ£€æŸ¥å‰é¢çš„é‡å¤æ¨¡å¼
                    if pos > 0:
                        # é˜²æ­¢è¿ç»­ç›¸åŒæ°¨åŸºé…¸
                        prev_token = seq[pos-1]
                        adjustment[b, pos, prev_token] -= anti_repeat_strength
                    
                    if pos > 1:
                        # é˜²æ­¢ä¸‰è¿é‡å¤
                        if seq[pos-1] == seq[pos-2]:
                            repeat_token = seq[pos-1]
                            adjustment[b, pos, repeat_token] -= anti_repeat_strength * 2
        
        return logits + adjustment
    
    def evaluate_sequence_quality(self, sequences: List[str]) -> Dict:
        """è¯„ä¼°åºåˆ—è´¨é‡"""
        if not sequences:
            return {"error": "No sequences provided"}
        
        # ç»Ÿè®¡æ°¨åŸºé…¸åˆ†å¸ƒ
        all_aa = ''.join(sequences)
        aa_counts = Counter(all_aa)
        total_aa = len(all_aa)
        
        aa_distribution = {}
        for aa, count in aa_counts.items():
            aa_distribution[aa] = count / total_aa
        
        # è®¡ç®—ä¸ç›®æ ‡åˆ†å¸ƒçš„åå·®
        distribution_score = 0.0
        for aa, target_prob in self.target_distribution.items():
            actual_prob = aa_distribution.get(aa, 0.0)
            deviation = abs(actual_prob - target_prob)
            distribution_score += deviation
        
        # è®¡ç®—å¤šæ ·æ€§åˆ†æ•°
        unique_aa = len(set(all_aa))
        diversity_score = unique_aa / 20  # 20ç§æ ‡å‡†æ°¨åŸºé…¸
        
        # æ£€æŸ¥é‡å¤æ¨¡å¼
        repeat_patterns = 0
        for seq in sequences:
            for i in range(len(seq) - 2):
                if seq[i] == seq[i+1] == seq[i+2]:  # ä¸‰è¿é‡å¤
                    repeat_patterns += 1
        
        repeat_ratio = repeat_patterns / (len(sequences) * max(1, sum(len(s) for s in sequences) - 2))
        
        return {
            "total_sequences": len(sequences),
            "total_amino_acids": total_aa,
            "unique_amino_acids": unique_aa,
            "diversity_score": diversity_score,
            "distribution_deviation": distribution_score,
            "repeat_patterns": repeat_patterns,
            "repeat_ratio": repeat_ratio,
            "amino_acid_distribution": aa_distribution
        }


def test_improved_sampling():
    """æµ‹è¯•æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device("cpu")
    vocab_size = len(AMINO_ACID_VOCAB)
    
    scheduler = D3PMScheduler(num_timesteps=100, vocab_size=vocab_size)
    unet = D3PMUNet(vocab_size=vocab_size, hidden_dim=256, num_layers=4, max_seq_len=50)
    diffusion = D3PMDiffusion(unet, scheduler, device)
    
    print("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæ”¹è¿›çš„é‡‡æ ·å™¨
    sampler = ImprovedDiversitySampler()
    
    # æµ‹è¯•å‚æ•°
    batch_size = 4
    seq_len = 25
    num_batches = 5
    
    print(f"\nğŸ“Š æµ‹è¯•å‚æ•°:")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  - åºåˆ—é•¿åº¦: {seq_len}")
    print(f"  - æ‰¹æ¬¡æ•°é‡: {num_batches}")
    print(f"  - æ€»åºåˆ—æ•°: {batch_size * num_batches}")
    
    # æ ‡å‡†é‡‡æ ·
    print(f"\nğŸ² 1. æ ‡å‡†é‡‡æ ·:")
    standard_sequences = []
    for _ in range(num_batches):
        generated = diffusion.sample(batch_size=batch_size, seq_len=seq_len, 
                                   num_inference_steps=20, temperature=1.0)
        for seq_tokens in generated:
            seq = tokens_to_sequence(seq_tokens)
            if seq:  # åªæ·»åŠ éç©ºåºåˆ—
                standard_sequences.append(seq)
    
    print(f"æ ‡å‡†é‡‡æ ·ç”Ÿæˆäº† {len(standard_sequences)} ä¸ªæœ‰æ•ˆåºåˆ—")
    standard_quality = sampler.evaluate_sequence_quality(standard_sequences)
    
    # æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·
    print(f"\nğŸŒˆ 2. æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·:")
    diverse_sequences = []
    for _ in range(num_batches):
        generated = sampler.gentle_diverse_sample(
            diffusion, batch_size=batch_size, seq_len=seq_len,
            num_inference_steps=20, diversity_strength=0.1,
            temperature=1.05, anti_repeat_strength=0.15
        )
        for seq_tokens in generated:
            seq = tokens_to_sequence(seq_tokens)
            if seq:  # åªæ·»åŠ éç©ºåºåˆ—
                diverse_sequences.append(seq)
    
    print(f"æ”¹è¿›é‡‡æ ·ç”Ÿæˆäº† {len(diverse_sequences)} ä¸ªæœ‰æ•ˆåºåˆ—")
    diverse_quality = sampler.evaluate_sequence_quality(diverse_sequences)
    
    # å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“ˆ 3. è¯¦ç»†å¯¹æ¯”åˆ†æ:")
    print(f"")
    print(f"åºåˆ—è´¨é‡æŒ‡æ ‡:")
    print(f"  æ ‡å‡†é‡‡æ ·:")
    print(f"    - å¤šæ ·æ€§åˆ†æ•°: {standard_quality['diversity_score']:.3f}")
    print(f"    - åˆ†å¸ƒåå·®: {standard_quality['distribution_deviation']:.3f}")
    print(f"    - é‡å¤æ¨¡å¼: {standard_quality['repeat_patterns']}")
    print(f"    - é‡å¤æ¯”ä¾‹: {standard_quality['repeat_ratio']:.3f}")
    
    print(f"  æ”¹è¿›é‡‡æ ·:")
    print(f"    - å¤šæ ·æ€§åˆ†æ•°: {diverse_quality['diversity_score']:.3f}")
    print(f"    - åˆ†å¸ƒåå·®: {diverse_quality['distribution_deviation']:.3f}")
    print(f"    - é‡å¤æ¨¡å¼: {diverse_quality['repeat_patterns']}")
    print(f"    - é‡å¤æ¯”ä¾‹: {diverse_quality['repeat_ratio']:.3f}")
    
    # æ°¨åŸºé…¸åˆ†å¸ƒå¯¹æ¯”
    print(f"\næ°¨åŸºé…¸åˆ†å¸ƒå¯¹æ¯”:")
    all_aa = set(standard_quality['amino_acid_distribution'].keys()) | \
             set(diverse_quality['amino_acid_distribution'].keys())
    
    print(f"{'AA':>3} {'æ ‡å‡†':>8} {'æ”¹è¿›':>8} {'ç›®æ ‡':>8} {'æ”¹å–„':>8}")
    print("-" * 40)
    
    improvements = []
    for aa in sorted(all_aa):
        standard_pct = standard_quality['amino_acid_distribution'].get(aa, 0) * 100
        diverse_pct = diverse_quality['amino_acid_distribution'].get(aa, 0) * 100
        target_pct = sampler.target_distribution.get(aa, 0) * 100
        
        # è®¡ç®—æ”¹å–„ç¨‹åº¦ï¼ˆè·ç¦»ç›®æ ‡çš„åå·®æ˜¯å¦å‡å°ï¼‰
        standard_dev = abs(standard_pct - target_pct)
        diverse_dev = abs(diverse_pct - target_pct)
        improvement = (standard_dev - diverse_dev) / max(standard_dev, 0.001) * 100
        improvements.append(improvement)
        
        print(f"{aa:>3} {standard_pct:>7.1f}% {diverse_pct:>7.1f}% {target_pct:>7.1f}% {improvement:>+6.1f}%")
    
    # åºåˆ—ç¤ºä¾‹
    print(f"\nğŸ” 4. åºåˆ—ç¤ºä¾‹å¯¹æ¯”:")
    print(f"")
    print(f"æ ‡å‡†é‡‡æ ·ç¤ºä¾‹:")
    for i, seq in enumerate(standard_sequences[:5]):
        print(f"  {i+1:2d}. {seq}")
    
    print(f"\næ”¹è¿›é‡‡æ ·ç¤ºä¾‹:")
    for i, seq in enumerate(diverse_sequences[:5]):
        print(f"  {i+1:2d}. {seq}")
    
    # æ€»ç»“
    avg_improvement = np.mean(improvements)
    print(f"\n" + "=" * 60)
    if avg_improvement > 5:
        print(f"ğŸ‰ æ”¹è¿›æ•ˆæœæ˜¾è‘—ï¼å¹³å‡æ”¹å–„: {avg_improvement:+.1f}%")
    elif avg_improvement > 0:
        print(f"âœ… æ”¹è¿›æ•ˆæœæ¸©å’Œã€‚å¹³å‡æ”¹å–„: {avg_improvement:+.1f}%")
    else:
        print(f"âš ï¸  æ”¹è¿›æ•ˆæœæœ‰é™ã€‚å¹³å‡æ”¹å–„: {avg_improvement:+.1f}%")
    
    print(f"å»ºè®®:")
    if diverse_quality['distribution_deviation'] < standard_quality['distribution_deviation']:
        print(f"  - åˆ†å¸ƒåå·®æœ‰æ‰€æ”¹å–„ï¼Œå¯ç»§ç»­ä½¿ç”¨æ”¹è¿›é‡‡æ ·")
    if diverse_quality['repeat_ratio'] < standard_quality['repeat_ratio']:
        print(f"  - é‡å¤æ¨¡å¼å‡å°‘ï¼Œåºåˆ—è´¨é‡æå‡")
    
    print("=" * 60)


if __name__ == "__main__":
    test_improved_sampling()

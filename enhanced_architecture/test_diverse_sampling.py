#!/usr/bin/env python3
"""
æµ‹è¯•å¤šæ ·æ€§æ‰©æ•£é‡‡æ ·
éªŒè¯ç”˜æ°¨é…¸è¿‡åº¦ç”Ÿæˆé—®é¢˜æ˜¯å¦å¾—åˆ°è§£å†³
"""

import torch
import numpy as np
from collections import Counter
from data_loader import sequence_to_tokens, tokens_to_sequence, AMINO_ACID_VOCAB, VOCAB_TO_AA
from diffusion_models.d3pm_diffusion import D3PMDiffusion, D3PMScheduler, D3PMUNet

def analyze_sequences(sequences):
    """åˆ†æåºåˆ—çš„æ°¨åŸºé…¸åˆ†å¸ƒ"""
    all_aa = ""
    for seq in sequences:
        all_aa += seq
    
    aa_counts = Counter(all_aa)
    total_aa = len(all_aa)
    
    print(f"æ€»æ°¨åŸºé…¸æ•°: {total_aa}")
    print("æ°¨åŸºé…¸åˆ†å¸ƒ:")
    for aa, count in sorted(aa_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_aa * 100
        print(f"  {aa}: {count:4d} ({percentage:5.1f}%)")
    
    return aa_counts, total_aa

def test_diverse_generation():
    """æµ‹è¯•å¤šæ ·æ€§ç”Ÿæˆ"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•å¤šæ ·æ€§æ‰©æ•£é‡‡æ ·")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cpu")
    vocab_size = len(AMINO_ACID_VOCAB)
    
    # åˆ›å»ºæ¨¡å‹ç»„ä»¶
    scheduler = D3PMScheduler(num_timesteps=100, vocab_size=vocab_size)
    unet = D3PMUNet(vocab_size=vocab_size, hidden_dim=128, num_layers=4, 
                    max_seq_len=50)
    diffusion = D3PMDiffusion(unet, scheduler, device)
    
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # å‚æ•°è®¾ç½®
    batch_size = 5
    seq_len = 30
    num_samples = 20  # ç”Ÿæˆæ›´å¤šæ ·æœ¬è¿›è¡Œç»Ÿè®¡
    
    print(f"\nğŸ“Š ç”Ÿæˆå‚æ•°:")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  - åºåˆ—é•¿åº¦: {seq_len}")
    print(f"  - æ€»æ ·æœ¬æ•°: {num_samples}")
    
    # 1. æµ‹è¯•æ ‡å‡†é‡‡æ ·
    print(f"\nğŸ² 1. æ ‡å‡†é‡‡æ ·ç»“æœ:")
    standard_sequences = []
    for i in range(0, num_samples, batch_size):
        current_batch = min(batch_size, num_samples - i)
        generated = diffusion.sample(
            batch_size=current_batch, 
            seq_len=seq_len, 
            num_inference_steps=20,
            temperature=1.0
        )
        
        for j in range(current_batch):
            seq = tokens_to_sequence(generated[j])
            standard_sequences.append(seq)
    
    print(f"æ ‡å‡†é‡‡æ ·ç”Ÿæˆäº† {len(standard_sequences)} ä¸ªåºåˆ—")
    analyze_sequences(standard_sequences)
    
    # 2. æµ‹è¯•å¤šæ ·æ€§é‡‡æ ·
    print(f"\nğŸŒˆ 2. å¤šæ ·æ€§é‡‡æ ·ç»“æœ:")
    diverse_sequences = []
    for i in range(0, num_samples, batch_size):
        current_batch = min(batch_size, num_samples - i)
        generated = diffusion.diverse_sample(
            batch_size=current_batch, 
            seq_len=seq_len, 
            num_inference_steps=20,
            diversity_strength=0.5,
            temperature=1.2
        )
        
        for j in range(current_batch):
            seq = tokens_to_sequence(generated[j])
            diverse_sequences.append(seq)
    
    print(f"å¤šæ ·æ€§é‡‡æ ·ç”Ÿæˆäº† {len(diverse_sequences)} ä¸ªåºåˆ—")
    diverse_counts, diverse_total = analyze_sequences(diverse_sequences)
    
    # 3. å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“ˆ 3. å¯¹æ¯”åˆ†æ:")
    
    # è®¡ç®—ç”˜æ°¨é…¸æ¯”ä¾‹
    standard_g_count = sum(seq.count('G') for seq in standard_sequences)
    standard_total = sum(len(seq) for seq in standard_sequences)
    standard_g_ratio = standard_g_count / standard_total if standard_total > 0 else 0
    
    diverse_g_count = diverse_counts.get('G', 0)
    diverse_g_ratio = diverse_g_count / diverse_total if diverse_total > 0 else 0
    
    print(f"ç”˜æ°¨é…¸(G)æ¯”ä¾‹:")
    print(f"  æ ‡å‡†é‡‡æ ·: {standard_g_ratio:.1%}")
    print(f"  å¤šæ ·æ€§é‡‡æ ·: {diverse_g_ratio:.1%}")
    print(f"  æ”¹å–„æ¯”ä¾‹: {((standard_g_ratio - diverse_g_ratio) / standard_g_ratio * 100):.1f}%")
    
    # è®¡ç®—æ°¨åŸºé…¸å¤šæ ·æ€§ï¼ˆä½¿ç”¨çš„ä¸åŒæ°¨åŸºé…¸ç§ç±»ï¼‰
    standard_diversity = len(set(''.join(standard_sequences)))
    diverse_diversity = len(set(''.join(diverse_sequences)))
    
    print(f"\næ°¨åŸºé…¸å¤šæ ·æ€§:")
    print(f"  æ ‡å‡†é‡‡æ ·: {standard_diversity} ç§æ°¨åŸºé…¸")
    print(f"  å¤šæ ·æ€§é‡‡æ ·: {diverse_diversity} ç§æ°¨åŸºé…¸")
    
    # 4. å±•ç¤ºç¤ºä¾‹åºåˆ—
    print(f"\nğŸ” 4. åºåˆ—ç¤ºä¾‹å¯¹æ¯”:")
    print(f"\næ ‡å‡†é‡‡æ ·ç¤ºä¾‹:")
    for i, seq in enumerate(standard_sequences[:5]):
        print(f"  {i+1}. {seq}")
    
    print(f"\nå¤šæ ·æ€§é‡‡æ ·ç¤ºä¾‹:")
    for i, seq in enumerate(diverse_sequences[:5]):
        print(f"  {i+1}. {seq}")
    
    # 5. è¯„ä¼°ç”Ÿæˆè´¨é‡
    print(f"\nğŸ¯ 5. ç”Ÿæˆè´¨é‡è¯„ä¼°:")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡çŸ­çš„åºåˆ—
    standard_short = sum(1 for seq in standard_sequences if len(seq) < 10)
    diverse_short = sum(1 for seq in diverse_sequences if len(seq) < 10)
    
    print(f"è¿‡çŸ­åºåˆ—(< 10aa):")
    print(f"  æ ‡å‡†é‡‡æ ·: {standard_short}/{len(standard_sequences)} ({standard_short/len(standard_sequences)*100:.1f}%)")
    print(f"  å¤šæ ·æ€§é‡‡æ ·: {diverse_short}/{len(diverse_sequences)} ({diverse_short/len(diverse_sequences)*100:.1f}%)")
    
    # æ£€æŸ¥é‡å¤åºåˆ—
    standard_unique = len(set(standard_sequences))
    diverse_unique = len(set(diverse_sequences))
    
    print(f"\nåºåˆ—å”¯ä¸€æ€§:")
    print(f"  æ ‡å‡†é‡‡æ ·: {standard_unique}/{len(standard_sequences)} å”¯ä¸€ ({standard_unique/len(standard_sequences)*100:.1f}%)")
    print(f"  å¤šæ ·æ€§é‡‡æ ·: {diverse_unique}/{len(diverse_sequences)} å”¯ä¸€ ({diverse_unique/len(diverse_sequences)*100:.1f}%)")
    
    print(f"\n" + "=" * 60)
    if diverse_g_ratio < standard_g_ratio * 0.8:  # å¦‚æœç”˜æ°¨é…¸æ¯”ä¾‹å‡å°‘20%ä»¥ä¸Š
        print("ğŸ‰ å¤šæ ·æ€§é‡‡æ ·æˆåŠŸï¼ç”˜æ°¨é…¸è¿‡åº¦ç”Ÿæˆé—®é¢˜å¾—åˆ°æ”¹å–„")
        if diverse_diversity > standard_diversity:
            print("ğŸŒŸ æ°¨åŸºé…¸å¤šæ ·æ€§ä¹Ÿå¾—åˆ°æå‡")
    else:
        print("âš ï¸  å¤šæ ·æ€§é‡‡æ ·æ•ˆæœæœ‰é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°")
    print("=" * 60)

if __name__ == "__main__":
    test_diverse_generation()

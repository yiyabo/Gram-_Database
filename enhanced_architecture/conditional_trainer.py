#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€è®­ç»ƒå™¨
"""

import os
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import random
import logging
import sys
from typing import List, Dict
from tqdm import tqdm

# è§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜: å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.path
# è¿™ä½¿å¾—æ— è®ºä»å“ªé‡Œè¿è¡Œè„šæœ¬ï¼Œéƒ½èƒ½æ‰¾åˆ°gram_predictorç­‰é¡¶çº§åŒ…
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥æˆ‘ä»¬åˆ›å»ºçš„ç»„ä»¶
from gram_predictor.data.conditional_dataset import ConditionalDataset, load_sequences_from_file
from gram_predictor.diffusion_models.conditional_d3pm import ConditionalD3PMUNet, ConditionalD3PMDiffusion, D3PMScheduler
from enhanced_architecture.conditional_esm2_extractor import ConditionalESM2FeatureExtractor
from gram_predictor.data_loader import sequence_to_tokens # ç”¨äºå°†åºåˆ—è½¬æ¢ä¸ºtoken ID

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ConditionalTrainer:
    """ç”¨äºè®­ç»ƒæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå™¨"""
    
    def __init__(self, config: dict):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.best_val_loss = float('inf')
        self.current_epoch = 0 # ç”¨äºæ¢å¤è®­ç»ƒ
        self.output_dir = self.config.get("output_dir", "output/checkpoints")
        os.makedirs(self.output_dir, exist_ok=True)
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        logger.info(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {self.output_dir}")
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self._init_components()
        
    def _init_components(self):
        """åˆå§‹åŒ–æ¨¡å‹ã€æ•°æ®åŠ è½½å™¨ã€ä¼˜åŒ–å™¨ç­‰"""
        logger.info("æ­£åœ¨åˆå§‹åŒ–ç»„ä»¶...")
        
        # 1. ç‰¹å¾æå–å™¨
        self.feature_extractor = ConditionalESM2FeatureExtractor(
            model_name=self.config.get("esm_model", "facebook/esm2_t8_215M_UR50D"),
            condition_dim=self.config["condition_dim"],
            use_layers=self.config.get("esm_layers", [2, 4, 6])
        ).to(self.device)
        
        # 2. æ‰©æ•£æ¨¡å‹
        unet = ConditionalD3PMUNet(
            hidden_dim=self.config["hidden_dim"],
            num_layers=self.config["num_layers"],
            condition_dim=self.config["condition_dim"]
        )
        scheduler = D3PMScheduler(num_timesteps=self.config["num_timesteps"])
        self.diffusion_model = ConditionalD3PMDiffusion(unet, scheduler, self.device)
        
        # æ£€æŸ¥å¹¶åº”ç”¨DataParallelä»¥æ”¯æŒå¤šGPU
        if torch.cuda.device_count() > 1:
            logger.info(f"æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPUï¼Œä½¿ç”¨ DataParallelã€‚")
            self.diffusion_model.model = torch.nn.DataParallel(self.diffusion_model.model)
        
        # 3. æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        # æ ¹æ®æ–°çš„é€»è¾‘ï¼ŒåŠ è½½å¤šä¸ªæ•°æ®æ–‡ä»¶å¹¶åˆå¹¶
        all_sequences = []
        for file_path in self.config["data_files"]:
            all_sequences.extend(load_sequences_from_file(file_path))
        
        # å»é‡å¹¶æ‰“ä¹±
        unique_sequences = sorted(list(set(all_sequences)))
        random.shuffle(unique_sequences)
        logger.info(f"ä» {len(self.config['data_files'])} ä¸ªæ–‡ä»¶åŠ è½½äº† {len(unique_sequences)} æ¡ç‹¬ç«‹åºåˆ—ã€‚")

        # å°†æ•´ä¸ªæ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
        total_size = len(unique_sequences)
        val_ratio = self.config.get("val_ratio", 0.1)
        val_size = int(total_size * val_ratio)
        # ç¡®ä¿åœ¨æ ·æœ¬é‡å¾ˆå°‘æ—¶ï¼ŒéªŒè¯é›†è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬
        if total_size > 1 and val_size == 0:
            val_size = 1
        train_size = total_size - val_size
        
        train_sequences = unique_sequences[:train_size]
        val_sequences = unique_sequences[train_size:]
        
        logger.info(f"æ•°æ®é›†åˆ†å‰² -> è®­ç»ƒé›†: {len(train_sequences)}, éªŒè¯é›†: {len(val_sequences)}")

        # ä¸ºå¯¹æ¯”å­¦ä¹ åŠ è½½æ­£è´Ÿæ ·æœ¬
        if self.config.get("use_contrastive", True):
            positive_seqs = load_sequences_from_file(self.config["contrastive_positive_path"])
            negative_seqs = load_sequences_from_file(self.config["contrastive_negative_path"])
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„Datasetæ¥é…å¯¹
            class ContrastiveDataset(Dataset):
                def __init__(self, pos, neg):
                    self.pos = pos
                    self.neg = neg
                def __len__(self):
                    return len(self.pos)
                def __getitem__(self, idx):
                    # éšæœºé€‰æ‹©ä¸€ä¸ªè´Ÿæ ·æœ¬
                    return {
                        "positive": self.pos[idx],
                        "negative": random.choice(self.neg)
                    }
            
            contrastive_dataset = ContrastiveDataset(positive_seqs, negative_seqs)
            self.contrastive_loader = DataLoader(
                contrastive_dataset,
                batch_size=self.config["batch_size"],
                shuffle=True
            )
            logger.info(f"å¯¹æ¯”å­¦ä¹ æ•°æ®é›†åŠ è½½å®Œæˆ: {len(positive_seqs)} æ­£æ ·æœ¬, {len(negative_seqs)} è´Ÿæ ·æœ¬ã€‚")
        else:
            self.contrastive_loader = None

        train_dataset = ConditionalDataset(
            train_sequences,
            pairing_strategy=self.config.get("pairing_strategy", "random"),
            num_references=self.config.get("num_references", 1)
        )
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config["batch_size"],
            shuffle=True,
            collate_fn=self.collate_fn
        )
        
        val_dataset = ConditionalDataset(
            val_sequences,
            pairing_strategy=self.config.get("pairing_strategy", "random"),
            num_references=self.config.get("num_references", 1)
        )
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config["batch_size"],
            shuffle=False,
            collate_fn=self.collate_fn
        )
        
        # 4. ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        # ä¼˜åŒ–å™¨åˆ†ä¸ºä¸¤éƒ¨åˆ†
        # 1. æ‰©æ•£æ¨¡å‹ä¼˜åŒ–å™¨
        self.diffusion_optimizer = optim.AdamW(
            self.diffusion_model.model.parameters(),
            lr=self.config["learning_rate"]
        )
        self.diffusion_lr_scheduler = optim.lr_scheduler.StepLR(
            self.diffusion_optimizer, step_size=10, gamma=0.9
        )

        # 2. ESM-2å’Œå¯¹æ¯”å­¦ä¹ å¤´ä¼˜åŒ–å™¨ (å¦‚æœESM-2æ²¡æœ‰è¢«å†»ç»“)
        if not self.config.get("freeze_esm", False):
            esm_params = list(self.feature_extractor.esm_model.parameters()) + \
                         list(self.feature_extractor.contrastive_projection.parameters())
            self.esm_optimizer = optim.AdamW(
                esm_params,
                lr=self.config.get("esm_learning_rate", 1e-5) # é€šå¸¸ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
            )
            self.esm_lr_scheduler = optim.lr_scheduler.StepLR(
                self.esm_optimizer, step_size=10, gamma=0.95
            )
        else:
            self.esm_optimizer = None
        
        logger.info("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆã€‚")

    def collate_fn(self, batch: List[Dict[str, List[str]]]) -> Dict[str, torch.Tensor]:
        """
        è‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œç”¨äºå¤„ç†æ‰¹æ¬¡æ•°æ®ã€‚
        1. æå–æ¡ä»¶ç‰¹å¾ã€‚
        2. å°†ç›®æ ‡åºåˆ—è½¬æ¢ä¸ºtokenå¼ é‡ã€‚
        """
        target_seqs = [item['target_sequence'][0] for item in batch]
        ref_seqs_list = [item['reference_sequences'] for item in batch]
        
        # 1. æå–æ¡ä»¶ç‰¹å¾
        # æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬çš„å¤šä¸ªå‚è€ƒåºåˆ—å–å¹³å‡ç‰¹å¾
        batch_condition_features = []
        for ref_seqs in ref_seqs_list:
            if ref_seqs:
                # æå–ç‰¹å¾å¹¶å–å¹³å‡
                features = self.feature_extractor.extract_condition_features(ref_seqs)
                features = features.mean(dim=0)
                batch_condition_features.append(features)
            else:
                # å¦‚æœæ²¡æœ‰å‚è€ƒåºåˆ—ï¼Œä½¿ç”¨é›¶å‘é‡æˆ–ç‰¹æ®Šçš„nullç‰¹å¾
                batch_condition_features.append(torch.zeros(self.config["condition_dim"], device=self.device))
        
        condition_features = torch.stack(batch_condition_features).to(self.device)
        
        # 2. å¤„ç†ç›®æ ‡åºåˆ—
        # å‡è®¾æ‰€æœ‰åºåˆ—é•¿åº¦ç›¸åŒï¼Œæˆ–è€…éœ€è¦è¿›è¡Œå¡«å……
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„æˆªæ–­/å¡«å……åˆ°å›ºå®šé•¿åº¦
        max_len = self.config.get("max_seq_len", 50)
        target_tokens_list = []
        for seq in target_seqs:
            tokens = sequence_to_tokens(seq, max_length=max_len)
            target_tokens_list.append(tokens)
            
        target_tokens = torch.stack(target_tokens_list).to(self.device)
        
        return {
            'target_tokens': target_tokens,
            'condition_features': condition_features
        }

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œä» Epoch {self.current_epoch + 1} å¼€å§‹...")
        
        for epoch in range(self.current_epoch, self.config["epochs"]):
            self.current_epoch = epoch # æ›´æ–°å½“å‰epoch
            self.diffusion_model.model.train()
            if self.esm_optimizer:
                self.feature_extractor.train()

            # ä½¿ç”¨zipæ¥åŒæ—¶è¿­ä»£ä¸¤ä¸ªdataloader
            loaders = {"diffusion": self.train_loader}
            if self.contrastive_loader:
                loaders["contrastive"] = self.contrastive_loader
            
            progress_bar = tqdm(zip(*loaders.values()), desc=f"Epoch {epoch + 1}/{self.config['epochs']}", total=len(self.train_loader))
            
            epoch_losses = {"diffusion": [], "contrastive": []}

            for batch_tuple in progress_bar:
                batch_dict = dict(zip(loaders.keys(), batch_tuple))

                # --- 1. æ‰©æ•£æ¨¡å‹è®­ç»ƒæ­¥éª¤ ---
                diffusion_batch = batch_dict['diffusion']
                self.diffusion_optimizer.zero_grad()
                
                target_tokens = diffusion_batch['target_tokens']
                condition_features = diffusion_batch['condition_features']
                
                diffusion_loss = self.diffusion_model.training_loss(target_tokens, condition_features)
                diffusion_loss.backward()
                self.diffusion_optimizer.step()
                epoch_losses["diffusion"].append(diffusion_loss.item())

                # --- 2. å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ­¥éª¤ ---
                if self.esm_optimizer and 'contrastive' in batch_dict:
                    contrastive_batch = batch_dict['contrastive']
                    self.esm_optimizer.zero_grad()
                    
                    contrastive_loss = self.feature_extractor.compute_contrastive_loss(
                        contrastive_batch["positive"],
                        contrastive_batch["negative"]
                    )
                    
                    # åŠ æƒå¹¶åå‘ä¼ æ’­
                    contrastive_weight = self.config.get("contrastive_loss_weight", 0.1)
                    weighted_contrastive_loss = contrastive_loss * contrastive_weight
                    weighted_contrastive_loss.backward()
                    self.esm_optimizer.step()
                    epoch_losses["contrastive"].append(contrastive_loss.item())

                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix(
                    diff_loss=f"{epoch_losses['diffusion'][-1]:.4f}",
                    cont_loss=f"{epoch_losses['contrastive'][-1] if epoch_losses['contrastive'] else 0:.4f}"
                )

            # --- Epoch ç»“æŸ ---
            avg_diff_loss = sum(epoch_losses["diffusion"]) / len(epoch_losses["diffusion"])
            avg_cont_loss = sum(epoch_losses["contrastive"]) / len(epoch_losses["contrastive"]) if epoch_losses["contrastive"] else 0
            
            self.diffusion_lr_scheduler.step()
            if self.esm_optimizer:
                self.esm_lr_scheduler.step()
            
            logger.info(f"Epoch {epoch+1} å®Œæˆ | å¹³å‡æ‰©æ•£æŸå¤±: {avg_diff_loss:.4f} | å¹³å‡å¯¹æ¯”æŸå¤±: {avg_cont_loss:.4f}")
            
            # è¿è¡ŒéªŒè¯
            if self.val_loader:
                val_loss = self.validate()
                logger.info(f"Epoch {epoch+1} | éªŒè¯æŸå¤±: {val_loss:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.save_checkpoint(f'best_model_epoch_{epoch+1}.pt')
            
            # å®šæœŸä¿å­˜æœ€æ–°æ¨¡å‹
            if (epoch + 1) % self.config.get("save_interval", 5) == 0:
                self.save_checkpoint(f'checkpoint_epoch_{epoch+1}.pt')
            
        logger.info("âœ… è®­ç»ƒå®Œæˆï¼")

    def validate(self):
        """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        self.diffusion_model.model.eval()
        total_val_loss = 0.0
        
        with torch.no_grad():
            for batch in self.val_loader:
                target_tokens = batch['target_tokens']
                condition_features = batch['condition_features']
                
                loss = self.diffusion_model.training_loss(target_tokens, condition_features)
                total_val_loss += loss.item()
                
        return total_val_loss / len(self.val_loader)

    def save_checkpoint(self, filename: str):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint_path = os.path.join(self.output_dir, filename)
        
        # åªä¿å­˜æ¨¡å‹çš„state_dict
        # å¤„ç†DataParallelçš„æ¨¡å‹
        model_state_dict = self.diffusion_model.model.module.state_dict() \
            if isinstance(self.diffusion_model.model, torch.nn.DataParallel) \
            else self.diffusion_model.model.state_dict()

        torch.save({
            'epoch': self.current_epoch,
            'model_state_dict': model_state_dict,
            'diffusion_optimizer_state_dict': self.diffusion_optimizer.state_dict(),
            'esm_optimizer_state_dict': self.esm_optimizer.state_dict() if self.esm_optimizer else None,
            'diffusion_lr_scheduler_state_dict': self.diffusion_lr_scheduler.state_dict(),
            'esm_lr_scheduler_state_dict': self.esm_lr_scheduler.state_dict() if self.esm_lr_scheduler else None,
            'best_val_loss': self.best_val_loss,
        }, checkpoint_path)
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        # ç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„å®Œæ•´è·¯å¾„
        if not os.path.exists(checkpoint_path):
            logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶æœªæ‰¾åˆ°: {checkpoint_path}")
            return
            
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # æ™ºèƒ½å¤„ç†æ˜¯å¦ç”±DataParallelåŒ…è£…çš„æ¨¡å‹
        model_to_load = self.diffusion_model.model.module \
            if isinstance(self.diffusion_model.model, torch.nn.DataParallel) \
            else self.diffusion_model.model
            
        model_to_load.load_state_dict(checkpoint['model_state_dict'])
        self.diffusion_optimizer.load_state_dict(checkpoint['diffusion_optimizer_state_dict'])
        self.diffusion_lr_scheduler.load_state_dict(checkpoint['diffusion_lr_scheduler_state_dict'])
        
        if self.esm_optimizer and checkpoint.get('esm_optimizer_state_dict'):
            self.esm_optimizer.load_state_dict(checkpoint['esm_optimizer_state_dict'])
            self.esm_lr_scheduler.load_state_dict(checkpoint['esm_lr_scheduler_state_dict'])

        self.current_epoch = checkpoint.get('epoch', 0) + 1 # ä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        logger.info(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆã€‚å°†ä» Epoch {self.current_epoch} ç»§ç»­è®­ç»ƒã€‚")


if __name__ == '__main__':
    # å¯¼å…¥ argparse ä»¥ä¾¿åœ¨ __main__ ä¸­ä½¿ç”¨
    import argparse

    parser = argparse.ArgumentParser(description="æ¡ä»¶æ‰©æ•£æ¨¡å‹è®­ç»ƒè„šæœ¬")
    parser.add_argument("--resume_from", type=str, default=None,
                        help="ä»æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„æ¢å¤è®­ç»ƒã€‚")
    args = parser.parse_args()

    # æ ¹æ®æ“ä½œç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©é…ç½®
    if sys.platform == "darwin":
        # macOS (æœ¬åœ°) é…ç½®: ä½¿ç”¨å°æ¨¡å‹å’ŒCPU
        logger.info("æ£€æµ‹åˆ°macOSç³»ç»Ÿï¼Œä½¿ç”¨æœ¬åœ°æµ‹è¯•é…ç½®ã€‚")
        config = {
            "esm_model": "facebook/esm2_t6_8M_UR50D",
            "condition_dim": 320,
            "hidden_dim": 128,
            "num_layers": 2,
            "num_timesteps": 100,
            "max_seq_len": 30,
            "data_files": ["mock_neg_only.txt", "mock_both.txt"], # ä½¿ç”¨æ–‡ä»¶åˆ—è¡¨
            "val_ratio": 0.2, # æœ¬åœ°æµ‹è¯•ä½¿ç”¨æ›´å¤§çš„éªŒè¯é›†æ¯”ä¾‹
            "output_dir": "test_checkpoints_local",
            "pairing_strategy": "random",
            "num_references": 2,
            "batch_size": 4,
            "learning_rate": 1e-4,
            "epochs": 2,
            "save_interval": 1,
        }
    else:
        # Linux (æœåŠ¡å™¨) é…ç½®: ä½¿ç”¨å¤§æ¨¡å‹å’ŒGPU
        logger.info("æ£€æµ‹åˆ°Linuxç³»ç»Ÿï¼Œä½¿ç”¨æœåŠ¡å™¨è®­ç»ƒé…ç½®ã€‚")
        config = {
            "esm_model": "facebook/esm2_t33_650M_UR50D",
            "condition_dim": 512,
            "hidden_dim": 512,
            "num_layers": 8,
            "num_timesteps": 1000,
            "max_seq_len": 100,
            "data_files": [
                "enhanced_architecture/gram_neg_only.txt",
                "enhanced_architecture/gram_both.txt"
            ],
            "val_ratio": 0.1,
            "output_dir": "checkpoints_hybrid_650M", # æ–°çš„è¾“å‡ºç›®å½•
            "pairing_strategy": "similarity",
            "num_references": 3,
            "batch_size": 16,
            "learning_rate": 5e-5, # æ‰©æ•£æ¨¡å‹å­¦ä¹ ç‡
            "epochs": 200,
            "save_interval": 5,
            
            # --- æ–°å¢ï¼šæ··åˆè®­ç»ƒé…ç½® ---
            "use_contrastive": True,
            "freeze_esm": False, # æˆ‘ä»¬éœ€è¦å¾®è°ƒESM
            "esm_learning_rate": 1e-5, # ä¸ºESMè®¾ç½®æ›´å°çš„å­¦ä¹ ç‡
            "contrastive_loss_weight": 0.1, # å¯¹æ¯”æŸå¤±çš„æƒé‡
            "contrastive_positive_path": "enhanced_architecture/gram_neg_only.txt",
            "contrastive_negative_path": "enhanced_architecture/gram_pos_only.txt",
        }

    # --- ä»¥ä¸‹ä¸ºæµ‹è¯•æ‰§è¡Œé€»è¾‘ ---
    
    # å¦‚æœæ˜¯æœ¬åœ°æµ‹è¯•ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æ–‡ä»¶
    if sys.platform == "darwin":
        mock_neg_only_sequences = [("N" * 25) for _ in range(10)]
        mock_both_sequences = [("B" * 25) for _ in range(5)]
        
        with open("mock_neg_only.txt", 'w') as f:
            for seq in mock_neg_only_sequences:
                f.write(seq + "\n")
                
        with open("mock_both.txt", 'w') as f:
            for seq in mock_both_sequences:
                f.write(seq + "\n")
    else:
        # åœ¨æœåŠ¡å™¨ä¸Šï¼Œæˆ‘ä»¬å‡è®¾çœŸå®æ•°æ®æ–‡ä»¶å·²å­˜åœ¨
        # å¯ä»¥æ·»åŠ æ£€æŸ¥ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        for file_path in config["data_files"]:
            if not os.path.exists(file_path):
                logger.error(f"æœåŠ¡å™¨æ¨¡å¼ä¸‹ï¼Œæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
                logger.error("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å·²å‡†å¤‡å¥½ï¼Œæˆ–åœ¨æœ¬åœ°æ¨¡å¼ä¸‹è¿è¡Œä»¥ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ã€‚")
                sys.exit(1) # é€€å‡ºè„šæœ¬
            
    # è¿è¡Œè®­ç»ƒå™¨
    trainer = ConditionalTrainer(config)
    
    # å¦‚æœæä¾›äº†æ¢å¤è·¯å¾„ï¼Œåˆ™åŠ è½½æ£€æŸ¥ç‚¹
    if args.resume_from:
        trainer.load_checkpoint(args.resume_from)
        
    trainer.train()

    # ä»¥ä¸‹çš„éªŒè¯å’Œæ¸…ç†é€»è¾‘åªåœ¨æœ¬åœ°æµ‹è¯•æ—¶æ‰§è¡Œ
    if sys.platform == "darwin":
        try:
            logger.info("å¼€å§‹æ‰§è¡Œæœ¬åœ°æµ‹è¯•éªŒè¯...")
            # éªŒè¯æ£€æŸ¥ç‚¹æ˜¯å¦å·²åˆ›å»º
            assert os.path.exists(os.path.join(config["output_dir"], "best_model.pt"))
            # æœ¬åœ°æµ‹è¯•è¿è¡Œ2ä¸ªepochï¼Œsave_intervalæ˜¯1ï¼Œæ‰€ä»¥epoch_1å’Œepoch_2éƒ½åº”è¯¥å­˜åœ¨
            assert os.path.exists(os.path.join(config["output_dir"], "checkpoint_epoch_1.pt"))
            assert os.path.exists(os.path.join(config["output_dir"], "checkpoint_epoch_2.pt"))
            logger.info("âœ… æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼")
            
            # æµ‹è¯•åŠ è½½åŠŸèƒ½
            logger.info("æµ‹è¯•åŠ è½½æ£€æŸ¥ç‚¹...")
            new_trainer = ConditionalTrainer(config)
            new_trainer.load_checkpoint("best_model.pt")
            logger.info("âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼")
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æµ‹è¯•éªŒè¯å¤±è´¥: {e}", exc_info=True)
        finally:
            # æ¸…ç†æ¨¡æ‹Ÿæ–‡ä»¶å’Œç›®å½•
            import shutil
            if os.path.exists("mock_neg_only.txt"):
                os.remove("mock_neg_only.txt")
            if os.path.exists("mock_both.txt"):
                os.remove("mock_both.txt")
            
            # ç¡®ä¿åªåœ¨æœ¬åœ°æµ‹è¯•æ—¶æ‰æ¸…ç†è¾“å‡ºç›®å½•
            if os.path.exists(config["output_dir"]):
                shutil.rmtree(config["output_dir"])
            logger.info("å·²æ¸…ç†æ‰€æœ‰æœ¬åœ°æµ‹è¯•çš„æ¨¡æ‹Ÿæ–‡ä»¶å’Œç›®å½•ã€‚")
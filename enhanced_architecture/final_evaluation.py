#!/usr/bin/env python3
"""
æœ€ç»ˆæ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·è¯„ä¼°æ¨¡å‹æ€§èƒ½
éªŒè¯ä¿®å¤åçš„æ‰©æ•£æ¨¡å‹æ˜¯å¦æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚
"""

import torch
import numpy as np
from data_loader import sequence_to_tokens, tokens_to_sequence, AMINO_ACID_VOCAB
from diffusion_models.d3pm_diffusion import D3PMDiffusion, D3PMScheduler, D3PMUNet
from collections import Counter
import logging
from datetime import datetime
import os

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def analyze_sequence_properties(sequences):
    """åˆ†æåºåˆ—çš„ç”Ÿç‰©å­¦ç‰¹æ€§"""
    properties = {
        'lengths': [],
        'amino_acid_counts': Counter(),
        'hydrophobic_ratio': [],
        'charged_ratio': [],
        'aromatic_ratio': []
    }
    
    # æ°¨åŸºé…¸åˆ†ç±»
    hydrophobic = set(['A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W'])
    charged = set(['K', 'R', 'H', 'D', 'E'])
    aromatic = set(['F', 'Y', 'W'])
    
    for seq in sequences:
        if len(seq) == 0:
            continue
            
        properties['lengths'].append(len(seq))
        
        # ç»Ÿè®¡æ°¨åŸºé…¸
        for aa in seq:
            properties['amino_acid_counts'][aa] += 1
        
        # è®¡ç®—å„ç§æ¯”ä¾‹
        total_aa = len(seq)
        hydrophobic_count = sum(1 for aa in seq if aa in hydrophobic)
        charged_count = sum(1 for aa in seq if aa in charged)
        aromatic_count = sum(1 for aa in seq if aa in aromatic)
        
        properties['hydrophobic_ratio'].append(hydrophobic_count / total_aa)
        properties['charged_ratio'].append(charged_count / total_aa)
        properties['aromatic_ratio'].append(aromatic_count / total_aa)
    
    return properties

def calculate_diversity_metrics(sequences):
    """è®¡ç®—åºåˆ—å¤šæ ·æ€§æŒ‡æ ‡"""
    if not sequences:
        return 0, 0
    
    # å»é‡è®¡ç®—å”¯ä¸€æ€§
    unique_sequences = set(sequences)
    uniqueness = len(unique_sequences) / len(sequences)
    
    # è®¡ç®—ç¼–è¾‘è·ç¦»å¤šæ ·æ€§
    diversity_scores = []
    for i in range(len(sequences)):
        for j in range(i+1, len(sequences)):
            # ç®€å•çš„Hammingè·ç¦»ï¼ˆå¯¹äºç­‰é•¿åºåˆ—ï¼‰
            if len(sequences[i]) == len(sequences[j]):
                distance = sum(c1 != c2 for c1, c2 in zip(sequences[i], sequences[j]))
                diversity_scores.append(distance / len(sequences[i]))
    
    avg_diversity = np.mean(diversity_scores) if diversity_scores else 0
    
    return uniqueness, avg_diversity

def final_evaluation():
    """è¿›è¡Œæœ€ç»ˆçš„ç»¼åˆè¯„ä¼°"""
    print("=" * 70)
    print("ğŸ¯ æœ€ç»ˆæ¨¡å‹è¯„ä¼° - ä½¿ç”¨æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·")
    print("=" * 70)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device("cpu")
    vocab_size = len(AMINO_ACID_VOCAB)
    
    scheduler = D3PMScheduler(num_timesteps=1000, vocab_size=vocab_size)
    unet = D3PMUNet(vocab_size=vocab_size, hidden_dim=768, num_layers=12, 
                    max_seq_len=100)
    diffusion = D3PMDiffusion(unet, scheduler, device)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    checkpoint_path = "./output/checkpoints/latest.pt"
    if os.path.exists(checkpoint_path):
        print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # åŠ è½½æ‰©æ•£æ¨¡å‹æƒé‡
        if 'diffusion_model_state_dict' in checkpoint:
            diffusion.model.load_state_dict(checkpoint['diffusion_model_state_dict'])
            print("âœ“ æ‰©æ•£æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ‰©æ•£æ¨¡å‹æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ (vocab_size: {vocab_size})")
    
    # è¯„ä¼°å‚æ•°
    num_samples = 50  # ç”Ÿæˆæ›´å¤šæ ·æœ¬è¿›è¡Œå…¨é¢è¯„ä¼°
    seq_length = 30   # é€‚ä¸­çš„åºåˆ—é•¿åº¦
    batch_size = 10
    
    print(f"\nğŸ“Š è¯„ä¼°å‚æ•°:")
    print(f"  - æ€»æ ·æœ¬æ•°: {num_samples}")
    print(f"  - åºåˆ—é•¿åº¦: {seq_length}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # ç”Ÿæˆåºåˆ—
    print(f"\nğŸ² ä½¿ç”¨æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·ç”Ÿæˆåºåˆ—...")
    all_sequences = []
    
    num_batches = (num_samples + batch_size - 1) // batch_size
    
    for batch_idx in range(num_batches):
        current_batch_size = min(batch_size, num_samples - batch_idx * batch_size)
        
        # ä½¿ç”¨å¤šæ ·æ€§é‡‡æ ·
        generated_tokens = diffusion.diverse_sample(
            batch_size=current_batch_size,
            seq_len=seq_length,
            diversity_strength=0.4,  # ä½¿ç”¨è¾ƒå¼ºçš„å¤šæ ·æ€§
            temperature=1.1  # ç•¥é«˜çš„æ¸©åº¦å¢åŠ éšæœºæ€§
        )
        
        # è½¬æ¢ä¸ºåºåˆ—
        for i in range(current_batch_size):
            seq = tokens_to_sequence(generated_tokens[i])
            if len(seq) > 0:  # åªä¿ç•™æœ‰æ•ˆåºåˆ—
                all_sequences.append(seq)
    
    print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(all_sequences)} ä¸ªæœ‰æ•ˆåºåˆ—")
    
    # 1. åŸºç¡€ç»Ÿè®¡
    print(f"\nğŸ“ˆ 1. åŸºç¡€ç»Ÿè®¡åˆ†æ:")
    print(f"  - æœ‰æ•ˆåºåˆ—æ•°: {len(all_sequences)}")
    print(f"  - æœ‰æ•ˆç‡: {len(all_sequences)/num_samples:.1%}")
    
    if not all_sequences:
        print("âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆåºåˆ—ï¼Œè¯„ä¼°ç»ˆæ­¢")
        return
    
    # åˆ†æåºåˆ—é•¿åº¦
    lengths = [len(seq) for seq in all_sequences]
    print(f"  - å¹³å‡é•¿åº¦: {np.mean(lengths):.1f}")
    print(f"  - é•¿åº¦èŒƒå›´: {min(lengths)}-{max(lengths)}")
    
    # 2. æ°¨åŸºé…¸åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ§¬ 2. æ°¨åŸºé…¸åˆ†å¸ƒåˆ†æ:")
    aa_counter = Counter()
    total_aa = 0
    
    for seq in all_sequences:
        for aa in seq:
            aa_counter[aa] += 1
            total_aa += 1
    
    print(f"  æ€»æ°¨åŸºé…¸æ•°: {total_aa}")
    print(f"  æ°¨åŸºé…¸åˆ†å¸ƒ (å‰10ä½):")
    
    for aa, count in aa_counter.most_common(10):
        percentage = count / total_aa * 100
        print(f"    {aa}: {count:4d} ({percentage:5.1f}%)")
    
    # ä¸ç›®æ ‡åˆ†å¸ƒæ¯”è¾ƒ
    target_distribution = {
        'K': 0.1136, 'G': 0.1045, 'L': 0.0897, 'R': 0.0888, 'A': 0.0719,
        'I': 0.0612, 'V': 0.0577, 'P': 0.0569, 'S': 0.0500, 'C': 0.0459,
        'F': 0.0436, 'T': 0.0362, 'N': 0.0320, 'Q': 0.0248, 'D': 0.0247,
        'E': 0.0238, 'W': 0.0216, 'Y': 0.0209, 'H': 0.0198, 'M': 0.0122
    }
    
    print(f"\n  ä¸ç›®æ ‡åˆ†å¸ƒçš„åå·®:")
    total_deviation = 0
    for aa in ['K', 'G', 'L', 'R', 'A']:  # æ£€æŸ¥ä¸»è¦æ°¨åŸºé…¸
        current_freq = aa_counter[aa] / total_aa if aa in aa_counter else 0
        target_freq = target_distribution.get(aa, 0)
        deviation = abs(current_freq - target_freq)
        total_deviation += deviation
        print(f"    {aa}: å½“å‰{current_freq:.3f} vs ç›®æ ‡{target_freq:.3f} (åå·®: {deviation:.3f})")
    
    print(f"  å¹³å‡åå·®: {total_deviation/5:.3f}")
    
    # 3. åºåˆ—å¤šæ ·æ€§åˆ†æ
    print(f"\nğŸŒˆ 3. åºåˆ—å¤šæ ·æ€§åˆ†æ:")
    uniqueness, avg_diversity = calculate_diversity_metrics(all_sequences)
    print(f"  - åºåˆ—å”¯ä¸€æ€§: {uniqueness:.3f}")
    print(f"  - å¹³å‡å¤šæ ·æ€§: {avg_diversity:.3f}")
    
    # 4. ç”Ÿç‰©å­¦ç‰¹æ€§åˆ†æ
    print(f"\nğŸ§¬ 4. ç”Ÿç‰©å­¦ç‰¹æ€§åˆ†æ:")
    properties = analyze_sequence_properties(all_sequences)
    
    print(f"  ç–æ°´æ€§æ°¨åŸºé…¸æ¯”ä¾‹: {np.mean(properties['hydrophobic_ratio']):.3f}")
    print(f"  å¸¦ç”µæ°¨åŸºé…¸æ¯”ä¾‹: {np.mean(properties['charged_ratio']):.3f}")
    print(f"  èŠ³é¦™æ€§æ°¨åŸºé…¸æ¯”ä¾‹: {np.mean(properties['aromatic_ratio']):.3f}")
    
    # 5. åºåˆ—ç¤ºä¾‹å±•ç¤º
    print(f"\nğŸ“‹ 5. ç”Ÿæˆåºåˆ—ç¤ºä¾‹ (å‰10ä¸ª):")
    for i, seq in enumerate(all_sequences[:10]):
        print(f"  {i+1:2d}. {seq}")
    
    # 6. è´¨é‡è¯„ä¼°
    print(f"\nâ­ 6. æ•´ä½“è´¨é‡è¯„ä¼°:")
    
    # è´¨é‡åˆ†æ•°è®¡ç®—
    length_score = 1.0 if 15 <= np.mean(lengths) <= 50 else 0.5
    diversity_score = min(1.0, avg_diversity * 2)  # å¤šæ ·æ€§åˆ†æ•°
    distribution_score = max(0, 1.0 - total_deviation/5 * 10)  # åˆ†å¸ƒåˆ†æ•°
    uniqueness_score = uniqueness
    
    overall_score = (length_score + diversity_score + distribution_score + uniqueness_score) / 4
    
    print(f"  - é•¿åº¦åˆç†æ€§: {length_score:.3f}")
    print(f"  - åºåˆ—å¤šæ ·æ€§: {diversity_score:.3f}")
    print(f"  - åˆ†å¸ƒå‡†ç¡®æ€§: {distribution_score:.3f}")
    print(f"  - åºåˆ—å”¯ä¸€æ€§: {uniqueness_score:.3f}")
    print(f"  - ğŸ“Š æ€»ä½“è´¨é‡åˆ†æ•°: {overall_score:.3f}")
    
    # ç»“è®º
    print(f"\n" + "=" * 70)
    if overall_score >= 0.8:
        print("ğŸ‰ è¯„ä¼°ç»“è®º: æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼")
        print("âœ… å»ºè®®ï¼šæ¨¡å‹å·²å‡†å¤‡å¥½ç”¨äºç”Ÿäº§ç¯å¢ƒ")
    elif overall_score >= 0.6:
        print("âœ… è¯„ä¼°ç»“è®º: æ¨¡å‹æ€§èƒ½è‰¯å¥½")
        print("ğŸ’¡ å»ºè®®ï¼šå¯ä»¥æŠ•å…¥ä½¿ç”¨ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–")
    else:
        print("âš ï¸ è¯„ä¼°ç»“è®º: æ¨¡å‹æ€§èƒ½éœ€è¦æ”¹è¿›")
        print("ğŸ”§ å»ºè®®ï¼šéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°æˆ–é‡æ–°è®­ç»ƒ")
    
    print(f"ğŸ“ æ”¹è¿›çš„å¤šæ ·æ€§é‡‡æ ·æ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡")
    print("=" * 70)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"final_evaluation_{timestamp}.txt"
    
    with open(results_file, 'w') as f:
        f.write(f"æœ€ç»ˆæ¨¡å‹è¯„ä¼°ç»“æœ - {timestamp}\n")
        f.write("=" * 50 + "\n")
        f.write(f"æ€»ä½“è´¨é‡åˆ†æ•°: {overall_score:.3f}\n")
        f.write(f"æœ‰æ•ˆåºåˆ—æ•°: {len(all_sequences)}\n")
        f.write(f"åºåˆ—å”¯ä¸€æ€§: {uniqueness:.3f}\n")
        f.write(f"å¹³å‡å¤šæ ·æ€§: {avg_diversity:.3f}\n")
        f.write(f"åˆ†å¸ƒåå·®: {total_deviation/5:.3f}\n")
        f.write("\nç”Ÿæˆåºåˆ—ç¤ºä¾‹:\n")
        for i, seq in enumerate(all_sequences[:20]):
            f.write(f"{i+1:2d}. {seq}\n")
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

if __name__ == "__main__":
    final_evaluation()

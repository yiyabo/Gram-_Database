#!/usr/bin/env python3
"""
ä¸“é—¨é’ˆå¯¹åŒ4090é…ç½®çš„è®­ç»ƒè„šæœ¬
ä¿®å¤å¤šGPUæ”¯æŒå’Œæ˜¾å­˜ä¼˜åŒ–
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import wandb
import json

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config.model_config import get_config
from esm2_auxiliary_encoder import ESM2AuxiliaryEncoder, ContrastiveLoss
from diffusion_models.d3pm_diffusion import D3PMDiffusion, D3PMScheduler, D3PMUNet
from data_loader import AntimicrobialPeptideDataset, ContrastiveAMPDataset, collate_contrastive_batch
from evaluation.evaluator import ModelEvaluator

class Dual4090Trainer:
    """ä¸“é—¨é’ˆå¯¹åŒ4090çš„è®­ç»ƒå™¨"""
    
    def __init__(self, config_name: str = "dual_4090"):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.config = get_config(config_name)
        
        # è®¾ç½®è®¾å¤‡å’Œå¤šGPU
        self.setup_device()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–å˜é‡
        self.esm2_encoder = None
        self.diffusion_model = None
        self.scheduler = None
        self.contrastive_loss = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.best_val_loss = float('inf')
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = None
        self.val_loader = None
        self.test_loader = None
        self.contrastive_loader = None
        
        # ä¼˜åŒ–å™¨
        self.esm_optimizer = None
        self.diffusion_optimizer = None
        self.lr_scheduler = None
        
        # ç›‘æ§å·¥å…·
        self.writer = None
        self.evaluator = None
        
        self.logger.info(f"åŒ4090è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œé…ç½®: {config_name}")
    
    def setup_device(self):
        """è®¾ç½®è®¾å¤‡å’Œå¤šGPUæ”¯æŒ"""
        if torch.cuda.is_available():
            self.num_gpus = torch.cuda.device_count()
            self.device = torch.device("cuda:0")
            
            print(f"ğŸ” æ£€æµ‹åˆ° {self.num_gpus} ä¸ªGPU:")
            for i in range(self.num_gpus):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            
            # æ¸…ç†GPUç¼“å­˜
            for i in range(self.num_gpus):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
            
            # è®¾ç½®å¤šGPUç­–ç•¥
            if self.num_gpus >= 2:
                self.use_multi_gpu = True
                print(f"âœ… å¯ç”¨å¤šGPUè®­ç»ƒ (ä½¿ç”¨ {self.num_gpus} ä¸ªGPU)")
                
                # è®¾ç½®CUDAç¯å¢ƒå˜é‡ä¼˜åŒ–
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
                
            else:
                self.use_multi_gpu = False
                print("ğŸ“± ä½¿ç”¨å•GPUè®­ç»ƒ")
        else:
            self.device = torch.device("cpu")
            self.num_gpus = 0
            self.use_multi_gpu = False
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = os.path.join(self.config.training.output_dir, "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        log_file = os.path.join(log_dir, f"dual_4090_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶"""
        self.logger.info("ğŸš€ åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶...")
        
        # 1. ESM-2è¾…åŠ©ç¼–ç å™¨
        self.logger.info(f"åŠ è½½ESM-2æ¨¡å‹: {self.config.esm2.model_name}")
        self.esm2_encoder = ESM2AuxiliaryEncoder(self.config.esm2)
        
        # 2. æ‰©æ•£æ¨¡å‹
        self.logger.info("åˆå§‹åŒ–D3PMæ‰©æ•£æ¨¡å‹...")
        self.scheduler = D3PMScheduler(
            num_timesteps=self.config.diffusion.num_timesteps,
            schedule_type=self.config.diffusion.schedule_type,
            vocab_size=self.config.diffusion.vocab_size
        )
        
        d3pm_unet = D3PMUNet(
            vocab_size=self.config.diffusion.vocab_size,
            max_seq_len=self.config.diffusion.max_seq_len,
            hidden_dim=self.config.diffusion.hidden_dim,
            num_layers=self.config.diffusion.num_layers,
            num_heads=self.config.diffusion.num_heads,
            dropout=self.config.diffusion.dropout
        )
        
        self.diffusion_model = D3PMDiffusion(
            model=d3pm_unet,
            scheduler=self.scheduler,
            device=self.device
        )
        
        # 3. ç§»åŠ¨åˆ°GPU
        self.esm2_encoder = self.esm2_encoder.to(self.device)
        self.diffusion_model.model = self.diffusion_model.model.to(self.device)
        
        # 4. å¤šGPUåŒ…è£…
        if self.use_multi_gpu:
            self.logger.info(f"ğŸ”— å¯ç”¨å¤šGPU DataParallel...")
            
            # ESM-2ç¼–ç å™¨å¤šGPUï¼ˆå¦‚æœä¸å†»ç»“ï¼‰
            if not self.config.esm2.freeze_esm:
                self.esm2_encoder = nn.DataParallel(self.esm2_encoder)
                self.logger.info("âœ… ESM-2ç¼–ç å™¨å·²å¯ç”¨DataParallel")
            
            # æ‰©æ•£æ¨¡å‹å¤šGPU
            self.diffusion_model.model = nn.DataParallel(self.diffusion_model.model)
            self.logger.info("âœ… æ‰©æ•£æ¨¡å‹å·²å¯ç”¨DataParallel")
        
        # 5. å¯¹æ¯”å­¦ä¹ æŸå¤±
        if self.config.esm2.use_contrastive_learning:
            self.contrastive_loss = ContrastiveLoss(
                temperature=self.config.esm2.contrastive_temperature
            )
        
        # 6. è¯„ä¼°å™¨
        self.evaluator = ModelEvaluator(self.config.evaluation)
        
        # è¾“å‡ºæ¨¡å‹ä¿¡æ¯
        esm_params = sum(p.numel() for p in self.esm2_encoder.parameters() if p.requires_grad)
        diffusion_params = sum(p.numel() for p in self.diffusion_model.model.parameters())
        
        self.logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        self.logger.info(f"  ESM-2ç¼–ç å™¨å¯è®­ç»ƒå‚æ•°: {esm_params:,}")
        self.logger.info(f"  æ‰©æ•£æ¨¡å‹å‚æ•°: {diffusion_params:,}")
        self.logger.info(f"  æ€»å‚æ•°é‡: {esm_params + diffusion_params:,}")
        
        # æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            for i in range(self.num_gpus):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                self.logger.info(f"  GPU {i} æ˜¾å­˜: {allocated:.2f}GB å·²åˆ†é…, {reserved:.2f}GB å·²ä¿ç•™, {total:.2f}GB æ€»è®¡")
    
    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        self.logger.info("ğŸ“ è®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        # ä¸»è®­ç»ƒæ•°æ®é›†
        main_dataset = AntimicrobialPeptideDataset(
            sequences_file=self.config.data.main_sequences_path,
            max_length=self.config.data.max_sequence_length
        )
        
        # å¯¹æ¯”å­¦ä¹ æ•°æ®é›†
        if self.config.esm2.use_contrastive_learning:
            contrastive_dataset = ContrastiveAMPDataset(
                positive_file=self.config.data.positive_sequences_path,
                negative_file=self.config.data.negative_sequences_path,
                max_length=self.config.data.max_sequence_length,
                negative_sample_ratio=self.config.esm2.negative_sample_ratio
            )
            
            self.contrastive_loader = DataLoader(
                contrastive_dataset,
                batch_size=self.config.esm2.batch_size,  # ä½¿ç”¨ESM-2ä¸“ç”¨æ‰¹æ¬¡å¤§å°
                shuffle=True,
                num_workers=self.config.data.num_workers,
                pin_memory=self.config.data.pin_memory,
                collate_fn=collate_contrastive_batch
            )
        
        # æ•°æ®é›†åˆ†å‰²
        total_size = len(main_dataset)
        train_size = int(self.config.data.train_ratio * total_size)
        val_size = int(self.config.data.val_ratio * total_size)
        test_size = total_size - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = random_split(
            main_dataset, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.data.batch_size,
            shuffle=True,
            num_workers=self.config.data.num_workers,
            pin_memory=self.config.data.pin_memory
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.data.batch_size,
            shuffle=False,
            num_workers=self.config.data.num_workers,
            pin_memory=self.config.data.pin_memory
        )
        
        self.test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.data.batch_size,
            shuffle=False,
            num_workers=self.config.data.num_workers,
            pin_memory=self.config.data.pin_memory
        )
        
        self.logger.info(f"ğŸ“Š æ•°æ®é›†å¤§å° - è®­ç»ƒ: {len(train_dataset)}, éªŒè¯: {len(val_dataset)}, æµ‹è¯•: {len(test_dataset)}")
        if self.contrastive_loader:
            self.logger.info(f"ğŸ“Š å¯¹æ¯”å­¦ä¹ æ•°æ®é›†å¤§å°: {len(contrastive_dataset)}")
    
    def setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        self.logger.info("âš™ï¸ è®¾ç½®ä¼˜åŒ–å™¨...")
        
        # ESM-2ä¼˜åŒ–å™¨
        esm_params = [p for p in self.esm2_encoder.parameters() if p.requires_grad]
        if esm_params:
            self.esm_optimizer = optim.AdamW(
                esm_params,
                lr=self.config.training.esm_learning_rate,
                weight_decay=self.config.training.weight_decay,
                betas=(0.9, 0.999)
            )
            self.logger.info(f"  ESM-2ä¼˜åŒ–å™¨: AdamW, lr={self.config.training.esm_learning_rate}")
        
        # æ‰©æ•£æ¨¡å‹ä¼˜åŒ–å™¨
        self.diffusion_optimizer = optim.AdamW(
            self.diffusion_model.model.parameters(),
            lr=self.config.training.learning_rate,
            weight_decay=self.config.training.weight_decay,
            betas=(0.9, 0.999)
        )
        self.logger.info(f"  æ‰©æ•£æ¨¡å‹ä¼˜åŒ–å™¨: AdamW, lr={self.config.training.learning_rate}")
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.training.use_lr_scheduler:
            total_steps = len(self.train_loader) * self.config.training.num_epochs
            self.lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.diffusion_optimizer,
                T_max=total_steps,
                eta_min=1e-7
            )
            self.logger.info(f"  å­¦ä¹ ç‡è°ƒåº¦å™¨: CosineAnnealingLR")
    
    def setup_monitoring(self):
        """è®¾ç½®ç›‘æ§å·¥å…·"""
        if self.config.training.use_tensorboard:
            log_dir = os.path.join(self.config.training.output_dir, "tensorboard")
            self.writer = SummaryWriter(log_dir)
            self.logger.info(f"ğŸ“ˆ TensorBoardæ—¥å¿—: {log_dir}")
        
        if self.config.training.use_wandb:
            wandb.init(
                project=self.config.training.wandb_project,
                config=self.config.__dict__,
                name=f"dual_4090_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            self.logger.info("ğŸ“Š WandBç›‘æ§å·²å¯ç”¨")
    
    def train_step(self, batch):
        """å•ä¸ªè®­ç»ƒæ­¥éª¤"""
        sequences = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        
        # æ‰©æ•£æ¨¡å‹è®­ç»ƒ
        loss = self.diffusion_model.training_loss(x_start=sequences)
        
        self.diffusion_optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            self.diffusion_model.model.parameters(),
            self.config.training.max_grad_norm
        )
        self.diffusion_optimizer.step()
        
        if self.lr_scheduler:
            self.lr_scheduler.step()
        
        return {'diffusion_loss': loss.item()}
    
    def contrastive_step(self, batch):
        """å¯¹æ¯”å­¦ä¹ æ­¥éª¤"""
        if not self.config.esm2.use_contrastive_learning:
            return {}
        
        try:
            positive_features, negative_features = self.esm2_encoder.extract_contrastive_features(
                batch['positive_sequences'], batch['negative_sequences']
            )
            
            contrastive_loss = self.contrastive_loss(positive_features, negative_features)
            
            if self.esm_optimizer:
                self.esm_optimizer.zero_grad()
                contrastive_loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.esm2_encoder.parameters(),
                    self.config.training.max_grad_norm
                )
                self.esm_optimizer.step()
            
            return {'contrastive_loss': contrastive_loss.item()}
        except Exception as e:
            self.logger.warning(f"å¯¹æ¯”å­¦ä¹ æ­¥éª¤å‡ºé”™: {e}")
            return {}
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.esm2_encoder.train()
        self.diffusion_model.model.train()
        
        epoch_losses = {'diffusion_loss': [], 'contrastive_loss': []}
        
        pbar = tqdm(
            enumerate(self.train_loader),
            total=len(self.train_loader),
            desc=f"Epoch {self.current_epoch + 1}/{self.config.training.num_epochs}"
        )
        
        for batch_idx, batch in pbar:
            self.global_step += 1
            
            # æ‰©æ•£æ¨¡å‹è®­ç»ƒ
            diffusion_losses = self.train_step(batch)
            epoch_losses['diffusion_loss'].append(diffusion_losses['diffusion_loss'])
            
            # å¯¹æ¯”å­¦ä¹ è®­ç»ƒ
            if self.config.esm2.use_contrastive_learning and self.contrastive_loader:
                try:
                    contrastive_batch = next(iter(self.contrastive_loader))
                    contrastive_losses = self.contrastive_step(contrastive_batch)
                    if contrastive_losses:
                        epoch_losses['contrastive_loss'].append(contrastive_losses['contrastive_loss'])
                except StopIteration:
                    pass
            
            # æ›´æ–°è¿›åº¦æ¡
            current_losses = {k: v[-1] if v else 0.0 for k, v in epoch_losses.items()}
            pbar.set_postfix(current_losses)
            
            # è®°å½•æŒ‡æ ‡
            if self.global_step % self.config.training.log_interval == 0:
                self.log_metrics(current_losses, prefix="train")
                
                # æ˜¾å­˜ç›‘æ§
                if torch.cuda.is_available() and self.global_step % (self.config.training.log_interval * 10) == 0:
                    for i in range(self.num_gpus):
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        self.logger.info(f"GPU {i} æ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB")
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {}
        for loss_name, loss_values in epoch_losses.items():
            if loss_values:
                avg_losses[f"avg_{loss_name}"] = np.mean(loss_values)
        
        return avg_losses
    
    def validate_epoch(self):
        """éªŒè¯epoch"""
        self.esm2_encoder.eval()
        self.diffusion_model.model.eval()
        
        val_losses = {'diffusion_loss': []}
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                sequences = batch['input_ids'].to(self.device)
                loss = self.diffusion_model.training_loss(x_start=sequences)
                val_losses['diffusion_loss'].append(loss.item())
        
        avg_val_loss = np.mean(val_losses['diffusion_loss'])
        return {'val_diffusion_loss': avg_val_loss}
    
    def log_metrics(self, metrics, prefix=""):
        """è®°å½•æŒ‡æ ‡"""
        if self.writer:
            for name, value in metrics.items():
                self.writer.add_scalar(f"{prefix}/{name}", value, self.global_step)
        
        if self.config.training.use_wandb:
            wandb_metrics = {f"{prefix}_{k}" if prefix else k: v for k, v in metrics.items()}
            wandb.log(wandb_metrics, step=self.global_step)
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # è·å–åŸå§‹æ¨¡å‹ï¼ˆå»é™¤DataParallelåŒ…è£…ï¼‰
        esm2_state = self.esm2_encoder.module.state_dict() if isinstance(self.esm2_encoder, nn.DataParallel) else self.esm2_encoder.state_dict()
        diffusion_state = self.diffusion_model.model.module.state_dict() if isinstance(self.diffusion_model.model, nn.DataParallel) else self.diffusion_model.model.state_dict()
        
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'esm2_encoder_state_dict': esm2_state,
            'diffusion_model_state_dict': diffusion_state,
            'esm_optimizer_state_dict': self.esm_optimizer.state_dict() if self.esm_optimizer else None,
            'diffusion_optimizer_state_dict': self.diffusion_optimizer.state_dict(),
            'lr_scheduler_state_dict': self.lr_scheduler.state_dict() if self.lr_scheduler else None,
            'config': self.config,
            'best_val_loss': self.best_val_loss
        }
        
        checkpoint_dir = os.path.join(self.config.training.output_dir, "checkpoints")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        latest_path = os.path.join(checkpoint_dir, "latest.pt")
        torch.save(checkpoint, latest_path)
        
        if is_best:
            best_path = os.path.join(checkpoint_dir, "best.pt")
            torch.save(checkpoint, best_path)
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        self.logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {latest_path}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("ğŸš€ å¼€å§‹åŒ4090è®­ç»ƒ...")
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self.initialize_models()
        self.setup_data_loaders()
        self.setup_optimizers()
        self.setup_monitoring()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.training.num_epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_losses = self.train_epoch()
            
            # éªŒè¯
            if epoch % self.config.training.val_interval == 0:
                val_losses = self.validate_epoch()
                self.log_metrics(val_losses, prefix="val")
                
                current_val_loss = val_losses['val_diffusion_loss']
                is_best = current_val_loss < self.best_val_loss
                if is_best:
                    self.best_val_loss = current_val_loss
                
                self.logger.info(
                    f"Epoch {epoch + 1}: "
                    f"è®­ç»ƒæŸå¤±: {train_losses.get('avg_diffusion_loss', 0):.4f}, "
                    f"éªŒè¯æŸå¤±: {current_val_loss:.4f}, "
                    f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}"
                )
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if epoch % self.config.training.save_interval == 0:
                    self.save_checkpoint(epoch, is_best)
        
        # è®­ç»ƒå®Œæˆ
        self.logger.info("âœ… è®­ç»ƒå®Œæˆ!")
        self.save_checkpoint(self.config.training.num_epochs - 1, False)
        
        if self.writer:
            self.writer.close()
        if self.config.training.use_wandb:
            wandb.finish()

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åŒ4090ä¼˜åŒ–è®­ç»ƒ")
    parser.add_argument("--config", type=str, default="dual_4090", help="é…ç½®åç§°")
    parser.add_argument("--resume", type=str, default=None, help="æ¢å¤è®­ç»ƒè·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Dual4090Trainer(config_name=args.config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()

if __name__ == "__main__":
    main()
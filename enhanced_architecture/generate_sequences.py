#!/usr/bin/env python3
"""
è®­ç»ƒå®Œæˆåçš„åºåˆ—ç”Ÿæˆè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–°çš„æŠ—èŒè‚½åºåˆ—
"""

import torch
import numpy as np
import argparse
from typing import List, Optional
import os

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config.model_config import get_config
from esm2_auxiliary_encoder import ESM2AuxiliaryEncoder
from diffusion_models.d3pm_diffusion import D3PMDiffusion, D3PMScheduler, D3PMUNet
from data_loader import tokens_to_sequence

class SequenceGenerator:
    """åºåˆ—ç”Ÿæˆå™¨"""
    
    def __init__(self, checkpoint_path: str, config_name: str = "dual_4090"):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            checkpoint_path: è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            config_name: é…ç½®åç§°
        """
        self.config = get_config(config_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åŠ è½½æ¨¡å‹
        self.load_models(checkpoint_path)
        
        print(f"âœ… åºåˆ—ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        print(f"ğŸ§¬ è¯æ±‡è¡¨å¤§å°: {self.config.diffusion.vocab_size}")
    
    def load_models(self, checkpoint_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹ - ä¿®å¤PyTorch 2.6çš„weights_onlyé—®é¢˜
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        
        # åˆå§‹åŒ–ESM-2ç¼–ç å™¨
        self.esm2_encoder = ESM2AuxiliaryEncoder(self.config.esm2)
        self.esm2_encoder.load_state_dict(checkpoint['esm2_encoder_state_dict'])
        self.esm2_encoder.to(self.device)
        self.esm2_encoder.eval()
        
        # åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹
        scheduler = D3PMScheduler(
            num_timesteps=self.config.diffusion.num_timesteps,
            schedule_type=self.config.diffusion.schedule_type,
            vocab_size=self.config.diffusion.vocab_size
        )
        
        unet = D3PMUNet(
            vocab_size=self.config.diffusion.vocab_size,
            max_seq_len=self.config.diffusion.max_seq_len,
            hidden_dim=self.config.diffusion.hidden_dim,
            num_layers=self.config.diffusion.num_layers,
            num_heads=self.config.diffusion.num_heads,
            dropout=self.config.diffusion.dropout
        )
        
        unet.load_state_dict(checkpoint['diffusion_model_state_dict'])
        
        self.diffusion_model = D3PMDiffusion(
            model=unet,
            scheduler=scheduler,
            device=self.device
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"ğŸ“Š è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
        print(f"ğŸ“ˆ æœ€ä½³éªŒè¯æŸå¤±: {checkpoint['best_val_loss']:.4f}")
    
    def generate_basic(self, num_sequences: int = 10, seq_length: int = 50, 
                      temperature: float = 1.0) -> List[str]:
        """
        åŸºç¡€åºåˆ—ç”Ÿæˆ
        
        Args:
            num_sequences: ç”Ÿæˆåºåˆ—æ•°é‡
            seq_length: åºåˆ—é•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦ (1.0=æ ‡å‡†, >1.0æ›´éšæœº, <1.0æ›´ç¡®å®š)
        
        Returns:
            ç”Ÿæˆçš„æ°¨åŸºé…¸åºåˆ—åˆ—è¡¨
        """
        print(f"ğŸ§¬ ç”Ÿæˆ {num_sequences} æ¡é•¿åº¦ä¸º {seq_length} çš„åºåˆ—...")
        
        with torch.no_grad():
            generated_tokens = self.diffusion_model.sample(
                batch_size=num_sequences,
                seq_len=seq_length,
                temperature=temperature
            )
        
        # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
        sequences = []
        for tokens in generated_tokens:
            seq = tokens_to_sequence(tokens.cpu().numpy())
            sequences.append(seq)
        
        return sequences
    
    def generate_with_reference(self, reference_sequences: List[str], 
                               num_sequences: int = 10, seq_length: int = 50,
                               temperature: float = 0.8) -> List[str]:
        """
        åŸºäºå‚è€ƒåºåˆ—çš„æ¡ä»¶ç”Ÿæˆ
        
        Args:
            reference_sequences: å‚è€ƒåºåˆ—åˆ—è¡¨
            num_sequences: ç”Ÿæˆåºåˆ—æ•°é‡
            seq_length: åºåˆ—é•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
        
        Returns:
            ç”Ÿæˆçš„æ°¨åŸºé…¸åºåˆ—åˆ—è¡¨
        """
        print(f"ğŸ¯ åŸºäº {len(reference_sequences)} æ¡å‚è€ƒåºåˆ—ç”Ÿæˆ...")
        
        # æå–ESM-2ç‰¹å¾
        with torch.no_grad():
            esm_features = self.esm2_encoder.encode_sequences(reference_sequences)
            # ä½¿ç”¨å¹³å‡ç‰¹å¾ä½œä¸ºæ¡ä»¶
            avg_features = esm_features.mean(dim=0, keepdim=True)
            
            generated_tokens = self.diffusion_model.sample(
                batch_size=num_sequences,
                seq_len=seq_length,
                esm_features=avg_features,
                temperature=temperature
            )
        
        # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
        sequences = []
        for tokens in generated_tokens:
            seq = tokens_to_sequence(tokens.cpu().numpy())
            sequences.append(seq)
        
        return sequences
    
    def generate_diverse(self, num_sequences: int = 10, seq_length: int = 50,
                        diversity_strength: float = 0.3, temperature: float = 1.0) -> List[str]:
        """
        å¤šæ ·æ€§æ„ŸçŸ¥ç”Ÿæˆ
        
        Args:
            num_sequences: ç”Ÿæˆåºåˆ—æ•°é‡
            seq_length: åºåˆ—é•¿åº¦
            diversity_strength: å¤šæ ·æ€§å¼ºåº¦ (0-1)
            temperature: é‡‡æ ·æ¸©åº¦
        
        Returns:
            ç”Ÿæˆçš„æ°¨åŸºé…¸åºåˆ—åˆ—è¡¨
        """
        print(f"ğŸŒˆ å¤šæ ·æ€§ç”Ÿæˆ {num_sequences} æ¡åºåˆ—...")
        
        with torch.no_grad():
            generated_tokens = self.diffusion_model.diverse_sample(
                batch_size=num_sequences,
                seq_len=seq_length,
                diversity_strength=diversity_strength,
                temperature=temperature
            )
        
        # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
        sequences = []
        for tokens in generated_tokens:
            seq = tokens_to_sequence(tokens.cpu().numpy())
            sequences.append(seq)
        
        return sequences
    
    def generate_high_quality(self, num_sequences: int = 10, seq_length: int = 50,
                             method: str = "top_k", **kwargs) -> List[str]:
        """
        é«˜è´¨é‡åºåˆ—ç”Ÿæˆ
        
        Args:
            num_sequences: ç”Ÿæˆåºåˆ—æ•°é‡
            seq_length: åºåˆ—é•¿åº¦
            method: é‡‡æ ·æ–¹æ³• ("top_k", "nucleus")
            **kwargs: é‡‡æ ·å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ°¨åŸºé…¸åºåˆ—åˆ—è¡¨
        """
        print(f"â­ ä½¿ç”¨ {method} æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡åºåˆ—...")
        
        with torch.no_grad():
            if method == "top_k":
                k = kwargs.get("k", 10)
                temperature = kwargs.get("temperature", 0.7)
                generated_tokens = self.diffusion_model.top_k_sample(
                    batch_size=num_sequences,
                    seq_len=seq_length,
                    k=k,
                    temperature=temperature
                )
            elif method == "nucleus":
                p = kwargs.get("p", 0.9)
                temperature = kwargs.get("temperature", 0.8)
                generated_tokens = self.diffusion_model.nucleus_sample(
                    batch_size=num_sequences,
                    seq_len=seq_length,
                    p=p,
                    temperature=temperature
                )
            else:
                raise ValueError(f"æœªçŸ¥çš„é‡‡æ ·æ–¹æ³•: {method}")
        
        # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
        sequences = []
        for tokens in generated_tokens:
            seq = tokens_to_sequence(tokens.cpu().numpy())
            sequences.append(seq)
        
        return sequences
    
    def batch_generate(self, total_sequences: int = 100, batch_size: int = 20,
                      seq_length: int = 50, output_file: str = "generated_sequences.txt") -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆåºåˆ—
        
        Args:
            total_sequences: æ€»åºåˆ—æ•°é‡
            batch_size: æ‰¹æ¬¡å¤§å°
            seq_length: åºåˆ—é•¿åº¦
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
        Returns:
            ç”Ÿæˆçš„æ°¨åŸºé…¸åºåˆ—åˆ—è¡¨
        """
        print(f"ğŸ”„ æ‰¹é‡ç”Ÿæˆ {total_sequences} æ¡åºåˆ—...")
        
        all_sequences = []
        
        for i in range(0, total_sequences, batch_size):
            current_batch = min(batch_size, total_sequences - i)
            
            # ä½¿ç”¨ä¸åŒçš„é‡‡æ ·ç­–ç•¥å¢åŠ å¤šæ ·æ€§
            if i % 3 == 0:
                sequences = self.generate_basic(current_batch, seq_length)
            elif i % 3 == 1:
                sequences = self.generate_high_quality(current_batch, seq_length, "top_k", k=15)
            else:
                sequences = self.generate_diverse(current_batch, seq_length)
            
            all_sequences.extend(sequences)
            print(f"  å·²ç”Ÿæˆ: {len(all_sequences)}/{total_sequences}")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w') as f:
            for i, seq in enumerate(all_sequences, 1):
                f.write(f">Generated_Sequence_{i}\n{seq}\n")
        
        print(f"ğŸ’¾ åºåˆ—å·²ä¿å­˜åˆ°: {output_file}")
        return all_sequences

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æŠ—èŒè‚½åºåˆ—ç”Ÿæˆå™¨")
    parser.add_argument("--checkpoint", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--config", type=str, default="dual_4090", help="é…ç½®åç§°")
    parser.add_argument("--mode", type=str, default="basic", 
                       choices=["basic", "reference", "diverse", "high_quality", "batch"],
                       help="ç”Ÿæˆæ¨¡å¼")
    parser.add_argument("--num_sequences", type=int, default=10, help="ç”Ÿæˆåºåˆ—æ•°é‡")
    parser.add_argument("--seq_length", type=int, default=50, help="åºåˆ—é•¿åº¦")
    parser.add_argument("--temperature", type=float, default=1.0, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--output", type=str, default="generated_sequences.txt", help="è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--reference", type=str, nargs="+", help="å‚è€ƒåºåˆ—")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = SequenceGenerator(args.checkpoint, args.config)
    
    # æ ¹æ®æ¨¡å¼ç”Ÿæˆåºåˆ—
    if args.mode == "basic":
        sequences = generator.generate_basic(
            num_sequences=args.num_sequences,
            seq_length=args.seq_length,
            temperature=args.temperature
        )
    
    elif args.mode == "reference":
        if not args.reference:
            # ä½¿ç”¨é»˜è®¤å‚è€ƒåºåˆ—
            reference_seqs = [
                "GLFDIVKKVVGALGSLGLVVR",  # å·²çŸ¥æŠ—é˜´æ€§èŒè‚½
                "KWVKAMDGVIDMLFYKMVYK",
                "FLGALFKALAALFVSSSK"
            ]
        else:
            reference_seqs = args.reference
        
        sequences = generator.generate_with_reference(
            reference_sequences=reference_seqs,
            num_sequences=args.num_sequences,
            seq_length=args.seq_length,
            temperature=args.temperature
        )
    
    elif args.mode == "diverse":
        sequences = generator.generate_diverse(
            num_sequences=args.num_sequences,
            seq_length=args.seq_length,
            temperature=args.temperature
        )
    
    elif args.mode == "high_quality":
        sequences = generator.generate_high_quality(
            num_sequences=args.num_sequences,
            seq_length=args.seq_length,
            method="top_k",
            k=10,
            temperature=0.7
        )
    
    elif args.mode == "batch":
        sequences = generator.batch_generate(
            total_sequences=args.num_sequences,
            seq_length=args.seq_length,
            output_file=args.output
        )
        return
    
    # æ˜¾ç¤ºç”Ÿæˆçš„åºåˆ—
    print(f"\nğŸ§¬ ç”Ÿæˆçš„åºåˆ—:")
    for i, seq in enumerate(sequences, 1):
        print(f"{i:2d}: {seq}")
    
    # ä¿å­˜åºåˆ—
    with open(args.output, 'w') as f:
        for i, seq in enumerate(sequences, 1):
            f.write(f">Generated_Sequence_{i}\n{seq}\n")
    
    print(f"\nğŸ’¾ åºåˆ—å·²ä¿å­˜åˆ°: {args.output}")

if __name__ == "__main__":
    main()
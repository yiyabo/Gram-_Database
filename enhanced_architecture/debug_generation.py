#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹
"""

import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.model_config import get_config
from main_trainer import EnhancedAMPTrainer
from data_loader import VOCAB_TO_AA

def debug_diffusion_generation():
    """è°ƒè¯•æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹"""
    
    print("ğŸ” å¼€å§‹è°ƒè¯•æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹...")
    
    # åŠ è½½é…ç½®å’Œæ¨¡å‹
    config = get_config("production")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    checkpoint_path = Path(config.training.output_dir) / "checkpoints" / "best.pt"
    if not checkpoint_path.exists():
        print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨å’Œæ¨¡å‹
    trainer = EnhancedAMPTrainer(config_name="production")
    trainer.initialize_models()
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    trainer.esm2_encoder.load_state_dict(checkpoint['esm2_encoder_state_dict'])
    trainer.diffusion_model.model.load_state_dict(checkpoint['diffusion_model_state_dict'])
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    trainer.diffusion_model.model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # è°ƒè¯•ç”Ÿæˆè¿‡ç¨‹
    batch_size = 2
    seq_len = 10
    
    with torch.no_grad():
        print(f"\nğŸ¯ å¼€å§‹ç”Ÿæˆè°ƒè¯• (batch_size={batch_size}, seq_len={seq_len})")
        
        # ä»éšæœºå™ªå£°å¼€å§‹
        x = torch.randint(0, trainer.scheduler.vocab_size, (batch_size, seq_len), device=device)
        print(f"åˆå§‹éšæœºå™ªå£°: {x}")
        print(f"åˆå§‹å™ªå£°èŒƒå›´: {x.min().item()} - {x.max().item()}")
        
        # ç®€åŒ–çš„é€†å‘æ‰©æ•£è¿‡ç¨‹ - åªæµ‹è¯•å‡ ä¸ªæ­¥éª¤
        num_inference_steps = 5
        timesteps = torch.linspace(trainer.scheduler.num_timesteps - 1, 0, 
                                 num_inference_steps, dtype=torch.long, device=device)
        
        print(f"\nğŸ“Š æ—¶é—´æ­¥åºåˆ—: {timesteps}")
        
        for i, t in enumerate(timesteps):
            t_batch = t.repeat(batch_size)
            print(f"\n--- æ­¥éª¤ {i+1}/{len(timesteps)}, æ—¶é—´æ­¥ t={t.item()} ---")
            print(f"å½“å‰ x: {x}")
            
            # è·å–æ¨¡å‹é¢„æµ‹çš„logits
            predicted_logits = trainer.diffusion_model.model(x, t_batch)
            print(f"é¢„æµ‹logitså½¢çŠ¶: {predicted_logits.shape}")
            print(f"logitsæ•°å€¼èŒƒå›´: {predicted_logits.min().item():.4f} - {predicted_logits.max().item():.4f}")
            
            # æ£€æŸ¥æ¯ä¸ªä½ç½®çš„logitsåˆ†å¸ƒ
            for pos in range(min(3, seq_len)):  # åªæ£€æŸ¥å‰3ä¸ªä½ç½®
                pos_logits = predicted_logits[0, pos, :]  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬posä¸ªä½ç½®
                pos_probs = F.softmax(pos_logits, dim=-1)
                top_k_probs, top_k_indices = torch.topk(pos_probs, k=5)
                
                print(f"  ä½ç½®{pos} - Top5æ¦‚ç‡:")
                for j, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices)):
                    aa = VOCAB_TO_AA[idx.item()]
                    print(f"    {j+1}. {aa} (token {idx.item()}): {prob.item():.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä½ç½®éƒ½å€¾å‘äºPAD
            argmax_tokens = torch.argmax(predicted_logits, dim=-1)
            print(f"argmax tokens: {argmax_tokens}")
            
            # è®¡ç®—PAD tokençš„å¹³å‡æ¦‚ç‡
            pad_probs = F.softmax(predicted_logits, dim=-1)[:, :, 0]  # PADæ˜¯token 0
            avg_pad_prob = pad_probs.mean().item()
            print(f"PAD tokenå¹³å‡æ¦‚ç‡: {avg_pad_prob:.4f}")
            
            # æ›´æ–°xç”¨äºä¸‹ä¸€æ­¥
            if i < len(timesteps) - 1:
                # ä½¿ç”¨å¤šé¡¹å¼é‡‡æ ·
                probs = F.softmax(predicted_logits, dim=-1)
                x = torch.multinomial(probs.view(-1, trainer.scheduler.vocab_size), 
                                    num_samples=1).view(batch_size, seq_len)
            else:
                # æœ€åä¸€æ­¥ä½¿ç”¨argmax
                x = torch.argmax(predicted_logits, dim=-1)
            
            print(f"æ›´æ–°åçš„ x: {x}")
        
        print(f"\nğŸ æœ€ç»ˆç”Ÿæˆç»“æœ:")
        print(f"æœ€ç»ˆtokens: {x}")
        
        # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
        from data_loader import tokens_to_sequence
        for i, seq_tokens in enumerate(x):
            seq = tokens_to_sequence(seq_tokens.cpu().numpy())
            print(f"åºåˆ— {i+1}: '{seq}' (é•¿åº¦: {len(seq)})")
        
    print("\nâœ… è°ƒè¯•å®Œæˆï¼")

def check_training_data_distribution():
    """æ£€æŸ¥è®­ç»ƒæ•°æ®çš„tokenåˆ†å¸ƒ"""
    print("\nğŸ“ˆ æ£€æŸ¥è®­ç»ƒæ•°æ®tokenåˆ†å¸ƒ...")
    
    from data_loader import AntimicrobialPeptideDataset
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    dataset = AntimicrobialPeptideDataset(
        sequences_file="main_training_sequences.txt",
        max_length=50
    )
    
    # ç»Ÿè®¡tokenåˆ†å¸ƒ
    token_counts = torch.zeros(21)  # 21ä¸ªtoken
    total_tokens = 0
    
    for i in range(min(100, len(dataset))):  # åªæ£€æŸ¥å‰100ä¸ªæ ·æœ¬
        sample = dataset[i]
        tokens = sample['input_ids']
        
        for token in tokens:
            if token.item() < 21:  # ç¡®ä¿åœ¨è¯æ±‡è¡¨èŒƒå›´å†…
                token_counts[token.item()] += 1
                total_tokens += 1
    
    # æ‰“å°åˆ†å¸ƒ
    print("Tokenåˆ†å¸ƒ:")
    for token_id in range(21):
        count = token_counts[token_id].item()
        percentage = (count / total_tokens) * 100 if total_tokens > 0 else 0
        aa = VOCAB_TO_AA[token_id]
        print(f"  {aa} (token {token_id}): {count} ({percentage:.2f}%)")
    
    print(f"æ€»tokenæ•°: {total_tokens}")

if __name__ == "__main__":
    print("ğŸ§ª æ‰©æ•£æ¨¡å‹ç”Ÿæˆè°ƒè¯•å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥è®­ç»ƒæ•°æ®åˆ†å¸ƒ
    check_training_data_distribution()
    
    print("\n" + "=" * 50)
    
    # è°ƒè¯•ç”Ÿæˆè¿‡ç¨‹
    debug_diffusion_generation()

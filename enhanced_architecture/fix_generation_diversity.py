#!/usr/bin/env python3
"""
ä¿®å¤æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·æ€§é—®é¢˜
è§£å†³ç”˜æ°¨é…¸è¿‡åº¦ç”Ÿæˆçš„é—®é¢˜
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Optional, Dict, List
from data_loader import AMINO_ACID_VOCAB, VOCAB_TO_AA, sequence_to_tokens, tokens_to_sequence

class DiversityAwareSampler:
    """å¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·å™¨ï¼Œé˜²æ­¢è¿‡åº¦ç”ŸæˆæŸäº›æ°¨åŸºé…¸"""
    
    def __init__(self, vocab_size: int = 21, target_distribution: Optional[Dict[str, float]] = None):
        self.vocab_size = vocab_size
        
        # è®¾ç½®ç›®æ ‡æ°¨åŸºé…¸åˆ†å¸ƒï¼ˆåŸºäºè®­ç»ƒæ•°æ®ï¼‰
        if target_distribution is None:
            self.target_distribution = {
                'K': 0.1136, 'G': 0.1045, 'L': 0.0897, 'R': 0.0888, 'A': 0.0719,
                'I': 0.0612, 'V': 0.0577, 'P': 0.0569, 'S': 0.0500, 'C': 0.0459,
                'F': 0.0436, 'T': 0.0362, 'N': 0.0320, 'Q': 0.0248, 'D': 0.0247,
                'E': 0.0238, 'W': 0.0216, 'Y': 0.0209, 'H': 0.0198, 'M': 0.0122
            }
        else:
            self.target_distribution = target_distribution
        
        # è½¬æ¢ä¸ºtokenåˆ†å¸ƒ
        self.target_token_probs = torch.zeros(vocab_size)
        for aa, prob in self.target_distribution.items():
            if aa in AMINO_ACID_VOCAB:
                token_id = AMINO_ACID_VOCAB[aa]
                self.target_token_probs[token_id] = prob
        
        # å½’ä¸€åŒ–
        self.target_token_probs = self.target_token_probs / self.target_token_probs.sum()
        
        print(f"âœ“ å¤šæ ·æ€§é‡‡æ ·å™¨åˆå§‹åŒ–ï¼Œç›®æ ‡åˆ†å¸ƒè®¾ç½®å®Œæˆ")
    
    def diverse_sampling(self, logits: torch.Tensor, generated_so_far: torch.Tensor, 
                        diversity_strength: float = 0.3, temperature: float = 1.0) -> torch.Tensor:
        """
        å¤šæ ·æ€§é‡‡æ ·ï¼Œè€ƒè™‘å½“å‰ç”Ÿæˆåºåˆ—çš„æ°¨åŸºé…¸åˆ†å¸ƒ
        
        Args:
            logits: æ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]
            generated_so_far: å·²ç”Ÿæˆçš„åºåˆ— [batch_size, current_len]
            diversity_strength: å¤šæ ·æ€§å¼ºåº¦ (0-1)
            temperature: é‡‡æ ·æ¸©åº¦
        """
        batch_size, seq_len, vocab_size = logits.shape
        device = logits.device
        
        # å°†ç›®æ ‡åˆ†å¸ƒç§»åˆ°æ­£ç¡®è®¾å¤‡
        target_probs = self.target_token_probs.to(device)
        
        # è®¡ç®—å½“å‰åºåˆ—çš„æ°¨åŸºé…¸åˆ†å¸ƒ
        current_distributions = []
        for i in range(batch_size):
            if generated_so_far.shape[1] > 0:
                # ç»Ÿè®¡å½“å‰åºåˆ—ä¸­å„æ°¨åŸºé…¸çš„å‡ºç°æ¬¡æ•°
                current_counts = torch.bincount(generated_so_far[i], minlength=vocab_size).float()
                current_dist = current_counts / (current_counts.sum() + 1e-8)
            else:
                current_dist = torch.zeros(vocab_size, device=device)
            current_distributions.append(current_dist)
        
        current_distributions = torch.stack(current_distributions)  # [batch_size, vocab_size]
        
        # è®¡ç®—åˆ†å¸ƒåå·®æƒ©ç½š
        distribution_penalty = torch.zeros_like(logits)
        for i in range(batch_size):
            for pos in range(seq_len):
                # å¯¹äºè¿‡åº¦å‡ºç°çš„æ°¨åŸºé…¸ï¼Œé™ä½å…¶æ¦‚ç‡
                overpresented_mask = current_distributions[i] > target_probs * 2  # è¶…è¿‡ç›®æ ‡2å€
                distribution_penalty[i, pos, overpresented_mask] = -diversity_strength * 2
                
                # å¯¹äºä¸è¶³çš„æ°¨åŸºé…¸ï¼Œæé«˜å…¶æ¦‚ç‡
                underpresented_mask = current_distributions[i] < target_probs * 0.5  # ä½äºç›®æ ‡ä¸€åŠ
                distribution_penalty[i, pos, underpresented_mask] = diversity_strength
        
        # åº”ç”¨å¤šæ ·æ€§æƒ©ç½š
        adjusted_logits = logits + distribution_penalty
        
        # æ¸©åº¦ç¼©æ”¾
        scaled_logits = adjusted_logits / temperature
        
        # é˜²æ­¢PAD tokenè¢«é€‰ä¸­
        scaled_logits[:, :, 0] = float('-inf')  # PAD token
        
        # é‡‡æ ·
        probs = F.softmax(scaled_logits, dim=-1)
        samples = torch.multinomial(probs.view(-1, vocab_size), num_samples=1).view(batch_size, seq_len)
        
        return samples
    
    def nucleus_sampling(self, logits: torch.Tensor, generated_so_far: torch.Tensor,
                        top_p: float = 0.9, diversity_strength: float = 0.2,
                        temperature: float = 1.0) -> torch.Tensor:
        """Nucleus (top-p) é‡‡æ ·withå¤šæ ·æ€§æ§åˆ¶"""
        batch_size, seq_len, vocab_size = logits.shape
        device = logits.device
        
        # åº”ç”¨å¤šæ ·æ€§è°ƒæ•´
        adjusted_logits = self._apply_diversity_adjustment(logits, generated_so_far, diversity_strength)
        
        # æ¸©åº¦ç¼©æ”¾
        scaled_logits = adjusted_logits / temperature
        
        # é˜²æ­¢PAD token
        scaled_logits[:, :, 0] = float('-inf')
        
        # åº”ç”¨nucleusé‡‡æ ·
        probs = F.softmax(scaled_logits, dim=-1)
        
        samples = torch.zeros(batch_size, seq_len, dtype=torch.long, device=device)
        
        for i in range(batch_size):
            for j in range(seq_len):
                # æ’åºæ¦‚ç‡
                sorted_probs, sorted_indices = torch.sort(probs[i, j], descending=True)
                
                # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
                cumulative_probs = torch.cumsum(sorted_probs, dim=0)
                
                # æ‰¾åˆ°nucleusè¾¹ç•Œ
                nucleus_mask = cumulative_probs <= top_p
                if nucleus_mask.sum() == 0:
                    nucleus_mask[0] = True  # è‡³å°‘ä¿ç•™ä¸€ä¸ª
                
                # ä»nucleusä¸­é‡‡æ ·
                nucleus_probs = sorted_probs[nucleus_mask]
                nucleus_indices = sorted_indices[nucleus_mask]
                
                # é‡æ–°å½’ä¸€åŒ–
                nucleus_probs = nucleus_probs / nucleus_probs.sum()
                
                # é‡‡æ ·
                selected_idx = torch.multinomial(nucleus_probs, num_samples=1)
                samples[i, j] = nucleus_indices[selected_idx]
        
        return samples
    
    def _apply_diversity_adjustment(self, logits: torch.Tensor, generated_so_far: torch.Tensor,
                                   diversity_strength: float) -> torch.Tensor:
        """åº”ç”¨å¤šæ ·æ€§è°ƒæ•´"""
        batch_size, seq_len, vocab_size = logits.shape
        device = logits.device
        target_probs = self.target_token_probs.to(device)
        
        # è®¡ç®—å½“å‰åˆ†å¸ƒ
        current_distributions = []
        for i in range(batch_size):
            if generated_so_far.shape[1] > 0:
                current_counts = torch.bincount(generated_so_far[i], minlength=vocab_size).float()
                current_dist = current_counts / (current_counts.sum() + 1e-8)
            else:
                current_dist = torch.zeros(vocab_size, device=device)
            current_distributions.append(current_dist)
        
        current_distributions = torch.stack(current_distributions)
        
        # è®¡ç®—è°ƒæ•´
        adjustment = torch.zeros_like(logits)
        for i in range(batch_size):
            for pos in range(seq_len):
                # å¯¹è¿‡åº¦å‡ºç°çš„æ°¨åŸºé…¸è¿›è¡Œæƒ©ç½š
                overpresented = current_distributions[i] > target_probs * 1.5
                adjustment[i, pos, overpresented] = -diversity_strength * 3
                
                # å¯¹ä¸è¶³çš„æ°¨åŸºé…¸è¿›è¡Œå¥–åŠ±
                underpresented = current_distributions[i] < target_probs * 0.3
                adjustment[i, pos, underpresented] = diversity_strength * 2
        
        return logits + adjustment


def test_diversity_sampler():
    """æµ‹è¯•å¤šæ ·æ€§é‡‡æ ·å™¨"""
    print("ğŸ§ª æµ‹è¯•å¤šæ ·æ€§é‡‡æ ·å™¨...")
    
    # åˆ›å»ºé‡‡æ ·å™¨
    sampler = DiversityAwareSampler()
    
    # æ¨¡æ‹Ÿä¸€äº›logits (åå‘G)
    batch_size, seq_len, vocab_size = 2, 10, 21
    logits = torch.randn(batch_size, seq_len, vocab_size)
    
    # äººä¸ºæé«˜G(token_id=8)çš„æ¦‚ç‡
    logits[:, :, 8] += 3.0  # å¤§å¹…æé«˜Gçš„logits
    
    # æ¨¡æ‹Ÿå·²ç”Ÿæˆçš„åºåˆ—ï¼ˆå¤§é‡Gï¼‰
    generated_so_far = torch.full((batch_size, 5), 8)  # å…¨æ˜¯G
    
    print("åŸå§‹logitsä¸­Gçš„ç›¸å¯¹æ¦‚ç‡:")
    probs = F.softmax(logits[0, 0], dim=0)
    print(f"G (token_8): {probs[8]:.3f}")
    
    # ä½¿ç”¨å¤šæ ·æ€§é‡‡æ ·
    diverse_samples = sampler.diverse_sampling(
        logits, generated_so_far, 
        diversity_strength=0.5, 
        temperature=1.0
    )
    
    print("\nå¤šæ ·æ€§é‡‡æ ·ç»“æœ:")
    for i in range(batch_size):
        sample_seq = tokens_to_sequence(diverse_samples[i])
        print(f"åºåˆ—{i+1}: {sample_seq}")
        
        # ç»Ÿè®¡æ°¨åŸºé…¸åˆ†å¸ƒ
        unique, counts = torch.unique(diverse_samples[i], return_counts=True)
        aa_counts = {}
        for token_id, count in zip(unique, counts):
            if token_id.item() in VOCAB_TO_AA:
                aa = VOCAB_TO_AA[token_id.item()]
                aa_counts[aa] = count.item()
        
        print(f"  æ°¨åŸºé…¸åˆ†å¸ƒ: {aa_counts}")
    
    print("âœ“ å¤šæ ·æ€§é‡‡æ ·å™¨æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_diversity_sampler()

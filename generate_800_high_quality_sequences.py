#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é«˜è´¨é‡æŠ—èŒè‚½åºåˆ—ç”Ÿæˆå™¨ - ç”Ÿæˆ800æ¡åºåˆ—
ä½¿ç”¨æ¡ä»¶D3PMæ¨¡å‹ + ESM-2ç‰¹å¾å¼•å¯¼ + Classifier-Free Guidance
ä¼˜åŒ–å‚æ•°è®¾ç½®ï¼Œç¡®ä¿é«˜è´¨é‡è¾“å‡º
"""

import os
import sys
import torch
import random
import argparse
import logging
import subprocess
from datetime import datetime

# è§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜
project_root = os.path.abspath(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥å¿…è¦ç»„ä»¶
try:
    from enhanced_architecture.conditional_esm2_extractor import ConditionalESM2FeatureExtractor
    from gram_predictor.diffusion_models.conditional_d3pm import ConditionalD3PMUNet, ConditionalD3PMDiffusion, D3PMScheduler
    from gram_predictor.data_loader import tokens_to_sequence
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'generation_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class HighQualitySequenceGenerator:
    """é«˜è´¨é‡åºåˆ—ç”Ÿæˆå™¨"""
    
    def __init__(self, checkpoint_path: str = None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # è‡ªåŠ¨æ£€æµ‹æ£€æŸ¥ç‚¹è·¯å¾„
        if checkpoint_path is None:
            checkpoint_path = self.find_best_checkpoint()
        
        self.checkpoint_path = checkpoint_path
        self.diffusion_model = None
        self.feature_extractor = None
        
        # ä¼˜åŒ–çš„æ¨¡å‹é…ç½® - åŸºäºæ‚¨æˆåŠŸçš„è®¾ç½®
        self.model_config = {
            "esm_model": "facebook/esm2_t33_650M_UR50D",
            "condition_dim": 512,
            "hidden_dim": 512,
            "num_layers": 8,
            "num_timesteps": 1000,
            "max_seq_len": 100,
        }
        
        # é«˜è´¨é‡ç”Ÿæˆå‚æ•°
        self.generation_params = {
            "guidance_scale": 5.0,  # é€‚ä¸­çš„å¼•å¯¼å¼ºåº¦ï¼Œé¿å…è¿‡æ‹Ÿåˆ
            "temperature": 0.8,     # ç¨ä½çš„æ¸©åº¦ï¼Œæé«˜è´¨é‡
            "min_len": 20,
            "max_len": 50,
            "num_references": 10,   # æ›´å¤šå‚è€ƒåºåˆ—
        }
        
    def find_best_checkpoint(self):
        """è‡ªåŠ¨æŸ¥æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹"""
        possible_paths = [
            "enhanced_architecture/output/checkpoints/best.pt",
            "enhanced_architecture/output/checkpoints/latest.pt",
            "gram_predictor/models/best.pt",
            "checkpoints/best.pt",
            "output/checkpoints/best.pt"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                logger.info(f"âœ… æ‰¾åˆ°æ£€æŸ¥ç‚¹: {path}")
                return path
        
        # æœç´¢æ‰€æœ‰.ptæ–‡ä»¶
        for root, dirs, files in os.walk("."):
            for file in files:
                if file.endswith(".pt") and ("best" in file or "checkpoint" in file):
                    full_path = os.path.join(root, file)
                    logger.info(f"ğŸ” å‘ç°æ£€æŸ¥ç‚¹: {full_path}")
                    return full_path
        
        raise FileNotFoundError("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼è¯·ç¡®ä¿å·²è®­ç»ƒæ¨¡å‹ã€‚")
    
    def load_models(self):
        """åŠ è½½æ¨¡å‹ç»„ä»¶"""
        logger.info(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {self.checkpoint_path}")
        
        if not os.path.exists(self.checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {self.checkpoint_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device, weights_only=False)
        logger.info(f"ğŸ“Š æ£€æŸ¥ç‚¹ä¿¡æ¯: Epoch {checkpoint.get('epoch', 'Unknown')}, Loss {checkpoint.get('best_val_loss', 'Unknown')}")
        
        # 1. åˆå§‹åŒ–ESM-2ç‰¹å¾æå–å™¨
        self.feature_extractor = ConditionalESM2FeatureExtractor(
            model_name=self.model_config["esm_model"],
            condition_dim=self.model_config["condition_dim"]
        ).to(self.device)
        
        # 2. åˆå§‹åŒ–æ¡ä»¶æ‰©æ•£æ¨¡å‹
        unet = ConditionalD3PMUNet(
            hidden_dim=self.model_config["hidden_dim"],
            num_layers=self.model_config["num_layers"],
            condition_dim=self.model_config["condition_dim"],
            max_seq_len=self.model_config["max_seq_len"]
        )
        
        scheduler = D3PMScheduler(num_timesteps=self.model_config["num_timesteps"])
        self.diffusion_model = ConditionalD3PMDiffusion(unet, scheduler, self.device)
        
        # 3. åŠ è½½æƒé‡
        try:
            # å¤„ç†å¯èƒ½çš„DataParallelåŒ…è£…
            model_state_dict = checkpoint['model_state_dict']
            if list(model_state_dict.keys())[0].startswith('module.'):
                from collections import OrderedDict
                new_state_dict = OrderedDict()
                for k, v in model_state_dict.items():
                    name = k[7:]  # ç§»é™¤'module.'å‰ç¼€
                    new_state_dict[name] = v
                model_state_dict = new_state_dict
            
            self.diffusion_model.model.load_state_dict(model_state_dict)
            logger.info("âœ… æ‰©æ•£æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
            raise
        
        logger.info("ğŸ¯ æ‰€æœ‰æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆ")
    
    def get_reference_sequences(self, num_refs: int):
        """è·å–é«˜è´¨é‡å‚è€ƒåºåˆ—"""
        reference_files = [
            "enhanced_architecture/gram_neg_only.txt",
            "enhanced_architecture/gram_both.txt"
        ]
        
        all_seqs = []
        for file_path in reference_files:
            if os.path.exists(file_path):
                with open(file_path, 'r') as f:
                    seqs = [line.strip() for line in f if line.strip() and len(line.strip()) >= 15]
                    all_seqs.extend(seqs)
        
        if not all_seqs:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å‚è€ƒåºåˆ—æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é«˜è´¨é‡åºåˆ—")
            # æä¾›ä¸€äº›é«˜è´¨é‡çš„é»˜è®¤å‚è€ƒåºåˆ—
            all_seqs = [
                "KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQATQIAK",
                "GIGKFLHSAKKFGKAFVGEIMNS",
                "KWKLFKKIGAVLKVLTTGLPALIS",
                "FLGALFKALKAA",
                "KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQATQIAK",
                "GIGKFLHSAKKFGKAFVGEIMNS",
                "KWKLFKKIGAVLKVLTTGLPALIS",
                "FLGALFKALKAA"
            ]
        
        if len(all_seqs) < num_refs:
            # å¦‚æœåºåˆ—ä¸å¤Ÿï¼Œé‡å¤ä½¿ç”¨
            all_seqs = all_seqs * ((num_refs // len(all_seqs)) + 1)
        
        selected = random.sample(all_seqs, min(num_refs, len(all_seqs)))
        logger.info(f"ğŸ“‹ é€‰æ‹©äº† {len(selected)} æ¡å‚è€ƒåºåˆ—")
        return selected
    
    def generate_sequences(self, num_sequences: int = 800):
        """ç”Ÿæˆé«˜è´¨é‡åºåˆ—"""
        logger.info(f"ğŸ¯ å¼€å§‹ç”Ÿæˆ {num_sequences} æ¡é«˜è´¨é‡åºåˆ—...")
        
        # åˆ†æ‰¹ç”Ÿæˆä»¥é¿å…å†…å­˜é—®é¢˜
        batch_size = 50  # æ¯æ‰¹ç”Ÿæˆ50æ¡
        all_sequences = []
        
        # è·å–å‚è€ƒåºåˆ—
        ref_sequences = self.get_reference_sequences(self.generation_params["num_references"])
        logger.info("å‚è€ƒåºåˆ—ç¤ºä¾‹: " + ", ".join(ref_sequences[:3]) + "...")
        
        # æå–æ¡ä»¶ç‰¹å¾
        with torch.no_grad():
            condition_features = self.feature_extractor.extract_condition_features(ref_sequences)
            final_condition = condition_features.mean(dim=0).unsqueeze(0)  # [1, condition_dim]
        
        num_batches = (num_sequences + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            current_batch_size = min(batch_size, num_sequences - batch_idx * batch_size)
            logger.info(f"ğŸ”„ ç”Ÿæˆæ‰¹æ¬¡ {batch_idx + 1}/{num_batches} ({current_batch_size} æ¡åºåˆ—)")
            
            # å¤åˆ¶æ¡ä»¶ä»¥åŒ¹é…æ‰¹æ¬¡å¤§å°
            batch_condition = final_condition.repeat(current_batch_size, 1)
            
            # éšæœºé€‰æ‹©åºåˆ—é•¿åº¦
            target_lengths = [random.randint(self.generation_params["min_len"], 
                                           self.generation_params["max_len"]) 
                            for _ in range(current_batch_size)]
            max_length = max(target_lengths)
            
            # ç”Ÿæˆåºåˆ—
            with torch.no_grad():
                generated_tokens = self.diffusion_model.sample(
                    batch_size=current_batch_size,
                    seq_len=max_length,
                    condition_features=batch_condition,
                    guidance_scale=self.generation_params["guidance_scale"],
                    temperature=self.generation_params["temperature"]
                )
            
            # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—å¹¶æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
            for i, (tokens, target_len) in enumerate(zip(generated_tokens, target_lengths)):
                full_sequence = tokens_to_sequence(tokens.cpu().numpy())
                truncated_sequence = full_sequence[:target_len]
                
                # åŸºæœ¬è´¨é‡æ£€æŸ¥
                if len(truncated_sequence) >= 10 and truncated_sequence.count('X') == 0:
                    all_sequences.append({
                        'id': f'HighQuality_Seq_{len(all_sequences) + 1:04d}',
                        'sequence': truncated_sequence,
                        'length': len(truncated_sequence),
                        'batch': batch_idx + 1
                    })
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(all_sequences)} æ¡åºåˆ—")
        return all_sequences
    
    def save_sequences(self, sequences, output_prefix="high_quality_800_sequences"):
        """ä¿å­˜ç”Ÿæˆçš„åºåˆ—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ä¸ºFASTAæ ¼å¼
        fasta_file = f"{output_prefix}_{timestamp}.fasta"
        with open(fasta_file, 'w') as f:
            for seq_data in sequences:
                f.write(f">{seq_data['id']} | Length={seq_data['length']} | Batch={seq_data['batch']}\n")
                f.write(f"{seq_data['sequence']}\n")
        
        # ä¿å­˜ä¸ºCSVæ ¼å¼
        csv_file = f"{output_prefix}_{timestamp}.csv"
        with open(csv_file, 'w') as f:
            f.write("ID,Sequence,Length,Batch\n")
            for seq_data in sequences:
                f.write(f"{seq_data['id']},{seq_data['sequence']},{seq_data['length']},{seq_data['batch']}\n")
        
        logger.info(f"ğŸ’¾ åºåˆ—å·²ä¿å­˜:")
        logger.info(f"   FASTA: {fasta_file}")
        logger.info(f"   CSV: {csv_file}")
        
        return fasta_file, csv_file
    
    def run_prediction(self, fasta_file):
        """è¿è¡Œé¢„æµ‹åˆ†æ"""
        logger.info("ğŸ” å¼€å§‹é¢„æµ‹åˆ†æ...")
        
        prediction_output = fasta_file.replace('.fasta', '_predictions.txt')
        
        # å°è¯•è°ƒç”¨é¢„æµ‹è„šæœ¬
        try:
            command = [
                sys.executable, "hybrid_predict.py",
                "--fasta_file", fasta_file,
                "--output_file", prediction_output
            ]
            
            result = subprocess.run(command, check=True, capture_output=True, text=True, timeout=1800)
            logger.info("âœ… é¢„æµ‹å®Œæˆ")
            logger.info(f"ğŸ“Š é¢„æµ‹ç»“æœä¿å­˜åˆ°: {prediction_output}")
            
            if result.stdout:
                logger.info("é¢„æµ‹è¾“å‡ºæ‘˜è¦:")
                for line in result.stdout.split('\n')[-10:]:  # æ˜¾ç¤ºæœ€å10è¡Œ
                    if line.strip():
                        logger.info(f"   {line}")
                        
        except subprocess.TimeoutExpired:
            logger.error("âŒ é¢„æµ‹è¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰")
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            if e.stdout:
                logger.error(f"æ ‡å‡†è¾“å‡º: {e.stdout}")
            if e.stderr:
                logger.error(f"æ ‡å‡†é”™è¯¯: {e.stderr}")
        except FileNotFoundError:
            logger.warning("âš ï¸ æœªæ‰¾åˆ° hybrid_predict.pyï¼Œè·³è¿‡é¢„æµ‹æ­¥éª¤")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é«˜è´¨é‡æŠ—èŒè‚½åºåˆ—ç”Ÿæˆå™¨")
    parser.add_argument("--checkpoint", type=str, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰")
    parser.add_argument("--num_sequences", type=int, default=800, help="ç”Ÿæˆåºåˆ—æ•°é‡")
    parser.add_argument("--guidance_scale", type=float, default=5.0, help="å¼•å¯¼å¼ºåº¦")
    parser.add_argument("--temperature", type=float, default=0.8, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--skip_prediction", action="store_true", help="è·³è¿‡é¢„æµ‹æ­¥éª¤")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = HighQualitySequenceGenerator(args.checkpoint)
        
        # æ›´æ–°å‚æ•°
        if args.guidance_scale != 5.0:
            generator.generation_params["guidance_scale"] = args.guidance_scale
        if args.temperature != 0.8:
            generator.generation_params["temperature"] = args.temperature
        
        # åŠ è½½æ¨¡å‹
        generator.load_models()
        
        # ç”Ÿæˆåºåˆ—
        sequences = generator.generate_sequences(args.num_sequences)
        
        if not sequences:
            logger.error("âŒ æœªç”Ÿæˆä»»ä½•åºåˆ—")
            return
        
        # ä¿å­˜åºåˆ—
        fasta_file, csv_file = generator.save_sequences(sequences)
        
        # è¿è¡Œé¢„æµ‹ï¼ˆå¯é€‰ï¼‰
        if not args.skip_prediction:
            generator.run_prediction(fasta_file)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        lengths = [seq['length'] for seq in sequences]
        logger.info("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡:")
        logger.info(f"   æ€»åºåˆ—æ•°: {len(sequences)}")
        logger.info(f"   å¹³å‡é•¿åº¦: {sum(lengths)/len(lengths):.1f}")
        logger.info(f"   é•¿åº¦èŒƒå›´: {min(lengths)}-{max(lengths)}")
        logger.info(f"   å¼•å¯¼å¼ºåº¦: {generator.generation_params['guidance_scale']}")
        logger.info(f"   é‡‡æ ·æ¸©åº¦: {generator.generation_params['temperature']}")
        
        logger.info("ğŸ‰ é«˜è´¨é‡åºåˆ—ç”Ÿæˆå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()